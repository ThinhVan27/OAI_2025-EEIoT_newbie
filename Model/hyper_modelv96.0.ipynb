{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageEnhance\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from skimage import color\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import ShuffleSplit, LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import cv2\n",
    "import copy\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training and test datasets with enhanced augmentation for 32x32 images...\n",
      "Using multi-augmentation strategy (5 copies of each image with different augmentations)\n",
      "Training samples: 4800\n",
      "Validation samples: 1200\n",
      "Test samples: 200\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "class_mapping = {\n",
    "    0: 1,\n",
    "    1: 3,\n",
    "    2: 0,\n",
    "    3: 2\n",
    "}\n",
    "\n",
    "train_dir = r\"D:\\6. OAI HCMC 2025\\aio-hutech\\train\"\n",
    "test_dir = r\"D:\\6. OAI HCMC 2025\\aio-hutech-\\test-label\"\n",
    "\n",
    "# Custom color space transformation\n",
    "class ColorSpaceTransform:\n",
    "    def __init__(self, p=0.3):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            img_np = np.array(img)\n",
    "            \n",
    "            # Choose one or more transformations\n",
    "            transform_type = random.choice(['hsv', 'lab', 'ycbcr'])\n",
    "            \n",
    "            if transform_type == 'hsv':\n",
    "                transformed = color.rgb2hsv(img_np)\n",
    "                hue_shift = np.random.uniform(-0.1, 0.1)\n",
    "                saturation_shift = np.random.uniform(0.8, 1.2)\n",
    "                transformed[:, :, 0] = (transformed[:, :, 0] + hue_shift) % 1.0\n",
    "                transformed[:, :, 1] = np.clip(transformed[:, :, 1] * saturation_shift, 0, 1)\n",
    "                transformed = color.hsv2rgb(transformed)\n",
    "                \n",
    "            elif transform_type == 'lab':\n",
    "                transformed = color.rgb2lab(img_np)\n",
    "                lightness_shift = np.random.uniform(0.8, 1.2)\n",
    "                transformed[:, :, 0] = np.clip(transformed[:, :, 0] * lightness_shift, 0, 100)\n",
    "                transformed = color.lab2rgb(transformed)\n",
    "                \n",
    "            elif transform_type == 'ycbcr':\n",
    "                transformed = color.rgb2ycbcr(img_np)\n",
    "                transformed = color.ycbcr2rgb(transformed)\n",
    "            \n",
    "            # Convert back to PIL\n",
    "            transformed = np.clip(transformed * 255, 0, 255).astype(np.uint8)\n",
    "            return Image.fromarray(transformed)\n",
    "        \n",
    "        return img\n",
    "\n",
    "# Mushroom-specific augmentation - adjusted for smaller images\n",
    "class MushroomSpecificAugment:\n",
    "    def __init__(self, p=0.4):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            img_np = np.array(img)\n",
    "            \n",
    "            if len(img_np.shape) != 3 or img_np.shape[2] != 3:\n",
    "                return img\n",
    "                \n",
    "            h, w, c = img_np.shape\n",
    "            \n",
    "            # Modify top part (mushroom cap)\n",
    "            top_region = img_np[:h//3, :, :]\n",
    "            \n",
    "            # Color augmentation\n",
    "            hue_shift = np.random.uniform(-0.1, 0.1)\n",
    "            saturation_shift = np.random.uniform(0.8, 1.2)\n",
    "            top_hsv = color.rgb2hsv(top_region)\n",
    "            top_hsv[:, :, 0] = (top_hsv[:, :, 0] + hue_shift) % 1.0\n",
    "            top_hsv[:, :, 1] = np.clip(top_hsv[:, :, 1] * saturation_shift, 0, 1)\n",
    "            top_region_new = color.hsv2rgb(top_hsv)\n",
    "            \n",
    "            img_np[:h//3, :, :] = np.clip(top_region_new * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "            # Add noise\n",
    "            if random.random() < 0.2:\n",
    "                noise = np.random.normal(0, 5, size=top_region.shape).astype(np.uint8)\n",
    "                img_np[:h//3, :, :] = np.clip(top_region + noise, 0, 255)\n",
    "\n",
    "            # Random erase part of the mushroom cap\n",
    "            if random.random() < 0.3:\n",
    "                erase_h = np.random.randint(5, h//3)\n",
    "                erase_w = np.random.randint(5, w)\n",
    "                img_np[:erase_h, :erase_w, :] = 0  # Erasing the cap\n",
    "\n",
    "            return Image.fromarray(img_np)\n",
    "        return img\n",
    "\n",
    "class MushroomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, is_test=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.samples = []\n",
    "        self.classes = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        if not os.path.exists(root_dir):\n",
    "            print(f\"Warning: Directory {root_dir} does not exist.\")\n",
    "            return\n",
    "        try:\n",
    "            items = os.listdir(root_dir)\n",
    "            if self.is_test or any(item.lower().endswith(('.png', '.jpg', '.jpeg')) for item in items):\n",
    "                self._setup_test_dataset(root_dir)\n",
    "            else:\n",
    "                self._setup_train_dataset(root_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up dataset: {e}\")\n",
    "            self.classes = [\"unknown\"]\n",
    "            self.class_to_idx = {\"unknown\": 0}\n",
    "\n",
    "    def _setup_test_dataset(self, root_dir):\n",
    "        \"\"\"Setup dataset for test directory (direct images without class subdirectories)\"\"\"\n",
    "        self.classes = [\"unknown\"]  # For testing, we don't know the class\n",
    "        self.class_to_idx = {\"unknown\": 0}\n",
    "\n",
    "        for img_name in os.listdir(root_dir):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                img_path = os.path.join(root_dir, img_name)\n",
    "                if os.path.isfile(img_path):\n",
    "                    self.samples.append((img_path, -1))  # Use -1 for test set\n",
    "\n",
    "    def _setup_train_dataset(self, root_dir):\n",
    "        \"\"\"Setup dataset for training directory (with class subdirectories)\"\"\"\n",
    "        self.classes = sorted([d for d in os.listdir(root_dir)\n",
    "                              if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                        self.samples.append((img_path, self.class_to_idx[class_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            placeholder = torch.zeros((3, 32, 32))  # Placeholder sized for 32x32 images\n",
    "            return placeholder, label\n",
    "\n",
    "# Enhanced dataset with multiple augmentations per image\n",
    "class MultiAugmentDataset(Dataset):\n",
    "    def __init__(self, dataset, num_copies=3):\n",
    "        self.dataset = dataset\n",
    "        self.num_copies = num_copies\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * self.num_copies\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = idx % len(self.dataset)\n",
    "        image, label = self.dataset[real_idx]\n",
    "        # Each copy will have different (random) augmentations\n",
    "        return image, label\n",
    "\n",
    "# Function to calculate mean and std\n",
    "def calculate_mean_std(dataset, batch_size=512):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)  # batch size (the last batch can have smaller size)\n",
    "        images = images.view(batch_samples, 3, -1)  # Flatten the images to shape (batch_size, 3, H*W)\n",
    "        mean += images.mean(2).mean(0)  # Mean over height and width\n",
    "        std += images.std(2).std(0)    # Std over height and width\n",
    "    mean /= len(loader)\n",
    "    std /= len(loader)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = calculate_mean_std(MushroomDataset(train_dir, transform=transforms.ToTensor()), batch_size=512)\n",
    "\n",
    "# Enhanced training transforms optimized for 32x32 images\n",
    "enhanced_train_transform = transforms.Compose([\n",
    "    # No initial resize needed as images are already 32x32\n",
    "    \n",
    "    # Geometric transformations - reduced scales for small images\n",
    "    transforms.RandomResizedCrop(32, scale=(0.7, 1.0)),  # Keep close to original size\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),  # Lower probability for vertical flip\n",
    "    transforms.RandomRotation(degrees=30),  # Reduced rotation angle\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=10),  # Gentler transformations\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.3),  # Reduced distortion\n",
    "    \n",
    "    # Color transformations\n",
    "    transforms.RandomGrayscale(p=0.1),  # Lower probability for grayscale\n",
    "    transforms.RandomSolarize(threshold=128, p=0.1),  # Lower probability for solarize\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),  # Gentler color jitter\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=1.5, p=0.3),  # Reduced sharpness\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.0)),  # Smaller kernel for small images\n",
    "    \n",
    "    # Custom mushroom-specific transforms\n",
    "    MushroomSpecificAugment(p=0.3),  # Reduced probability\n",
    "    ColorSpaceTransform(p=0.2),  # Reduced probability\n",
    "    \n",
    "    # Final processing\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15)),  # Smaller erasing regions\n",
    "    transforms.Normalize(mean.tolist(), std.tolist())\n",
    "])\n",
    "\n",
    "# Simplified evaluation transform for 32x32 images\n",
    "eval_transform = transforms.Compose([\n",
    "    # No resize needed as images are already 32x32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean.tolist(), std.tolist())\n",
    "])\n",
    "\n",
    "def setup_data_loaders(train_dir, test_dir, batch_size=512, val_split=0.2, use_multi_augment=True):\n",
    "    \"\"\"Setup the data loaders for training, validation, and testing with enhanced augmentation\"\"\"\n",
    "\n",
    "    print(\"Setting up training and test datasets with enhanced augmentation for 32x32 images...\")\n",
    "\n",
    "    if not os.path.exists(train_dir):\n",
    "        print(f\"Warning: Training directory {train_dir} does not exist!\")\n",
    "        os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "    # Create base dataset\n",
    "    train_dataset = MushroomDataset(train_dir, transform=enhanced_train_transform)\n",
    "    \n",
    "    # Apply multi-augmentation if requested\n",
    "    if use_multi_augment:\n",
    "        print(\"Using multi-augmentation strategy (5 copies of each image with different augmentations)\")\n",
    "        train_dataset = MultiAugmentDataset(train_dataset, num_copies=5)  \n",
    "    \n",
    "    # Create validation split\n",
    "    train_size = int((1 - val_split) * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "\n",
    "    train_dataset, valid_dataset = random_split(\n",
    "        train_dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    # Create test dataset\n",
    "    test_dataset = MushroomDataset(test_dir, transform=eval_transform, is_test=True)\n",
    "\n",
    "    # Create data loaders - using fewer workers for small images\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(valid_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "trainloader, valloader, testloader = setup_data_loaders(\n",
    "    train_dir, \n",
    "    test_dir,\n",
    "    batch_size=512,  # Increased batch size for smaller images\n",
    "    val_split=0.2,  # 20% for validation\n",
    "    use_multi_augment=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16_model(nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        vgg16 = models.vgg16(pretrained=pretrained)\n",
    "\n",
    "        self.features = vgg16.features\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 1 * 1, 4096),  \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = nn.AdaptiveAvgPool2d((1, 1))(x)  \n",
    "        x = x.view(x.size(0), -1)  \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class efficientnetb6_model(nn.Module):\n",
    "    def __init__(self, num_classes=4, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.efficientnetb6 = models.efficientnet_b6(weights=models.EfficientNet_B6_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "\n",
    "        self.features = nn.Sequential(*list(self.efficientnetb6.features.children()))\n",
    "\n",
    "        num_features = self.efficientnetb6.classifier[1].in_features  \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(num_features, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        pooled = self.efficientnetb6.avgpool(features)\n",
    "        x = torch.flatten(pooled, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\oai-hutech\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\oai-hutech\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "def mutual_traininng_model_v3(model1=efficientnetb6_model(4), model2=vgg16_model(4), model3=vgg16_model(4), \n",
    "                            train_loader=trainloader, valid_loader=valloader, \n",
    "                            num_epochs=15, learning_rate=0.001, device='cuda'):\n",
    "    \n",
    "    # Ensure models are on the correct device\n",
    "    model1 = model1.to(device)\n",
    "    model2 = model2.to(device)\n",
    "    model3 = model3.to(device)\n",
    "\n",
    "    # Define loss function and optimizers\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.2)\n",
    "\n",
    "    # Optimizers for each model\n",
    "    model1_optimizer = optim.AdamW(model1.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    model2_optimizer = optim.AdamW(model2.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    model3_optimizer = optim.AdamW(model3.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Scheduler for learning rate\n",
    "    scheduler_model1 = optim.lr_scheduler.OneCycleLR(\n",
    "        model1_optimizer,\n",
    "        max_lr=learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.3,\n",
    "        div_factor=25,\n",
    "        final_div_factor=1000,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    patience = 7\n",
    "    counter = 0\n",
    "\n",
    "    # Mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model1.train()\n",
    "        model2.train()\n",
    "        model3.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Clear gradients\n",
    "            model1_optimizer.zero_grad()\n",
    "            model2_optimizer.zero_grad()\n",
    "            model3_optimizer.zero_grad()\n",
    "\n",
    "            # Mutual learning: outputs from all three models\n",
    "            with torch.cuda.amp.autocast():\n",
    "                model1_outputs = model1(inputs)\n",
    "                model2_outputs = model2(inputs)\n",
    "                model3_outputs = model3(inputs)\n",
    "\n",
    "                # Softmax outputs to get pseudo-labels\n",
    "                soft_labels_from_model2 = torch.softmax(model2_outputs.detach(), dim=1)\n",
    "                soft_labels_from_model1 = torch.softmax(model1_outputs.detach(), dim=1)\n",
    "                soft_labels_from_model3 = torch.softmax(model3_outputs.detach(), dim=1)\n",
    "\n",
    "                # Compute loss for model1, model2, and model3 with mutual learning \n",
    "                model1_loss = criterion(model1_outputs, labels) + 0.5 * torch.mean(\n",
    "                    torch.sum(-soft_labels_from_model2 * torch.log_softmax(model1_outputs, dim=1), dim=1)\n",
    "                ) + 0.5 * torch.mean(\n",
    "                    torch.sum(-soft_labels_from_model3 * torch.log_softmax(model1_outputs, dim=1), dim=1)\n",
    "                )\n",
    "\n",
    "                model2_loss = criterion(model2_outputs, labels) + 0.5 * torch.mean(\n",
    "                    torch.sum(-soft_labels_from_model1 * torch.log_softmax(model2_outputs, dim=1), dim=1)\n",
    "                ) + 0.5 * torch.mean(\n",
    "                    torch.sum(-soft_labels_from_model3 * torch.log_softmax(model2_outputs, dim=1), dim=1)\n",
    "                )\n",
    "\n",
    "                model3_loss = criterion(model3_outputs, labels) + 0.5 * torch.mean(\n",
    "                    torch.sum(-soft_labels_from_model1 * torch.log_softmax(model3_outputs, dim=1), dim=1)\n",
    "                ) + 0.5 * torch.mean(\n",
    "                    torch.sum(-soft_labels_from_model2 * torch.log_softmax(model3_outputs, dim=1), dim=1)\n",
    "                )\n",
    "\n",
    "            # Backpropagation\n",
    "            scaler.scale(model1_loss).backward()\n",
    "            scaler.scale(model2_loss).backward()\n",
    "            scaler.scale(model3_loss).backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model1.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model2.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(model3.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Optimizer step\n",
    "            scaler.step(model1_optimizer)\n",
    "            scaler.step(model2_optimizer)\n",
    "            scaler.step(model3_optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, preds = torch.max(model1_outputs, 1)\n",
    "            running_loss += model1_loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data).item()\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        epoch_acc = running_corrects / total_samples\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "\n",
    "        # Validate the models\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        model3.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        val_total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Get outputs from all models\n",
    "                model1_outputs = model1(inputs)\n",
    "                model2_outputs = model2(inputs)\n",
    "                model3_outputs = model3(inputs)\n",
    "\n",
    "                # Calculate predictions from all models\n",
    "                model1_preds = torch.argmax(model1_outputs, dim=1)\n",
    "                model2_preds = torch.argmax(model2_outputs, dim=1)\n",
    "                model3_preds = torch.argmax(model3_outputs, dim=1)\n",
    "\n",
    "                # Calculate the loss for model1\n",
    "                loss = criterion(model1_outputs, labels)\n",
    "\n",
    "                # Calculate total validation loss and correct predictions\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += torch.sum(model1_preds == labels.data).item()\n",
    "                val_total_samples += inputs.size(0)\n",
    "\n",
    "        val_epoch_loss = val_running_loss / val_total_samples\n",
    "        val_epoch_acc = val_running_corrects / val_total_samples\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accs.append(val_epoch_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc*100:.2f}% - Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_epoch_acc*100:.2f}%\")\n",
    "        \n",
    "        scheduler_model1.step()\n",
    "        # scheduler_model2.step()\n",
    "        # scheduler_model3.step()\n",
    "        \n",
    "        # Save the best model\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            torch.save(model1.state_dict(), 'best_modelv1.pth')\n",
    "            torch.save(model2.state_dict(), 'best_modelv2.pth')\n",
    "            torch.save(model3.state_dict(), 'best_modelv3.pth')\n",
    "            print(f\"Saved best models with validation accuracy: {best_val_acc*100:.4f}\")\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss vs Epochs')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Val Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy vs Epochs')\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "    return model1, model2, model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_create_submission_ensemble(models, test_loader, class_mapping, filename='submission.csv'):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        models[i] = model.to(device)\n",
    "        models[i].eval()\n",
    "        print(f\"Model {i+1} running on: {next(models[i].parameters()).device}\")\n",
    "\n",
    "    all_preds = []\n",
    "    all_mapped_preds = []  \n",
    "    all_filenames = []\n",
    "\n",
    "    try:\n",
    "        test_dataset = test_loader.dataset\n",
    "        for i in range(len(test_dataset)):\n",
    "            if hasattr(test_dataset, 'samples') and i < len(test_dataset.samples):\n",
    "                img_path = test_dataset.samples[i][0]\n",
    "                filename_only = os.path.basename(img_path)\n",
    "                all_filenames.append(filename_only)\n",
    "            else:\n",
    "                all_filenames.append(f\"test_image_{i}.jpg\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Couldn't extract filenames from test dataset: {e}\")\n",
    "        all_filenames = [f\"test_image_{i}.jpg\" for i in range(len(test_loader.dataset))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, _) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            batch_preds = []\n",
    "            for model in models:\n",
    "                outputs = model(inputs)\n",
    "                probs = torch.softmax(outputs, dim=1) \n",
    "                batch_preds.append(probs)\n",
    "\n",
    "            ensemble_probs = sum(batch_preds) / len(models)\n",
    "            _, preds = torch.max(ensemble_probs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Processed {batch_idx}/{len(test_loader)} batches\")\n",
    "\n",
    "    for p in all_preds:\n",
    "        if p in class_mapping:\n",
    "            all_mapped_preds.append(class_mapping[p])\n",
    "        else:\n",
    "            print(f\"Warning: Prediction {p} not found in class_mapping. Using raw prediction.\")\n",
    "            all_mapped_preds.append(p)\n",
    "\n",
    "    formatted_ids = [os.path.splitext(fname)[0] for fname in all_filenames]\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': formatted_ids,\n",
    "        'label': all_mapped_preds\n",
    "    })\n",
    "\n",
    "    print(\"\\nSubmission sample (BEFORE saving):\")\n",
    "    print(submission_df.head(10))\n",
    "\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    print(f\"Submission file created: {filename}\")\n",
    "\n",
    "    print(\"\\nVerifying saved file by reading it back:\")\n",
    "    try:\n",
    "        saved_df = pd.read_csv(filename)\n",
    "        print(\"Sample from saved file:\")\n",
    "        print(saved_df.head(10))\n",
    "\n",
    "        if not submission_df.equals(saved_df):\n",
    "            print(\"WARNING: The saved file differs from the generated predictions!\")\n",
    "            diff_mask = submission_df != saved_df\n",
    "            diff_indices = diff_mask.any(axis=1)\n",
    "            if diff_indices.any():\n",
    "                print(\"Differences found at rows:\")\n",
    "                print(submission_df[diff_indices].compare(saved_df[diff_indices]))\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying saved file: {e}\")\n",
    "\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    expected_labels = []\n",
    "    expected_labels.extend([1] * 50)  \n",
    "    expected_labels.extend([2] * 50) \n",
    "    expected_labels.extend([3] * 50) \n",
    "    expected_labels.extend([0] * 50)  \n",
    "    \n",
    "    actual_labels = df['label'].tolist()\n",
    "    \n",
    "    n = min(len(actual_labels), len(expected_labels))\n",
    "    \n",
    "    correct = sum(1 for i in range(n) if actual_labels[i] == expected_labels[i])\n",
    "    \n",
    "    accuracy = (correct / n) * 100\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainingfull_programming(): \n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(f\"\\nCreating hypermodel-mutual-training models with {4} classes...\")\n",
    "    model1 = efficientnetb6_model(num_classes=4).to(device)\n",
    "    model2 = vgg16_model(num_classes=4).to(device)\n",
    "    model3 = vgg16_model(num_classes=4).to(device)\n",
    "    \n",
    "    print(\"\\n=== Starting model training ===\")\n",
    "    \n",
    "    trained_model, trained_aux1, trained_aux2 = mutual_traininng_model_v3(\n",
    "        model1=model1,\n",
    "        model2=model1,\n",
    "        model3=model1,\n",
    "        train_loader=trainloader,\n",
    "        valid_loader=valloader,\n",
    "        num_epochs=250,\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "     \n",
    "    print(\"\\n=== Making predictions on test data using ensemble of models ===\")\n",
    "    \n",
    "    model1.load_state_dict(torch.load('best_modelv1.pth'))\n",
    "    model2.load_state_dict(torch.load('best_modelv2.pth'))\n",
    "    model3.load_state_dict(torch.load('best_modelv3.pth'))\n",
    "\n",
    "    submission_df = predict_and_create_submission_ensemble(\n",
    "        models=[model1, model2, model3],\n",
    "        test_loader=testloader,\n",
    "        class_mapping=class_mapping,\n",
    "        filename='submission-dml-e-v96.csv',\n",
    "    )\n",
    "    \n",
    "    print(\"Test accuracy: \", calculate_accuracy(r\"D:\\6. OAI HCMC 2025\\aio-hutech-\\submission-dml-e-v96.csv\"))\n",
    "\n",
    "    print(\"\\n=== Process completed successfully! ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating hypermodel-mutual-training models with 4 classes...\n",
      "\n",
      "=== Starting model training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24752\\307943201.py:35: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24752\\307943201.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250 - Train Loss: 2.7772, Train Accuracy: 25.87% - Val Loss: 1.3892, Val Accuracy: 24.33%\n",
      "Saved best models with validation accuracy: 24.3333\n",
      "Epoch 2/250 - Train Loss: 2.7747, Train Accuracy: 26.27% - Val Loss: 1.3879, Val Accuracy: 24.33%\n",
      "Epoch 3/250 - Train Loss: 2.7728, Train Accuracy: 27.10% - Val Loss: 1.3882, Val Accuracy: 24.33%\n",
      "Epoch 4/250 - Train Loss: 2.7706, Train Accuracy: 27.85% - Val Loss: 1.3888, Val Accuracy: 24.42%\n",
      "Saved best models with validation accuracy: 24.4167\n",
      "Epoch 5/250 - Train Loss: 2.7660, Train Accuracy: 29.31% - Val Loss: 1.3906, Val Accuracy: 24.50%\n",
      "Saved best models with validation accuracy: 24.5000\n",
      "Epoch 6/250 - Train Loss: 2.7632, Train Accuracy: 30.25% - Val Loss: 1.3931, Val Accuracy: 24.25%\n",
      "Epoch 7/250 - Train Loss: 2.7617, Train Accuracy: 30.85% - Val Loss: 1.3942, Val Accuracy: 27.17%\n",
      "Saved best models with validation accuracy: 27.1667\n",
      "Epoch 8/250 - Train Loss: 2.7592, Train Accuracy: 31.31% - Val Loss: 1.3954, Val Accuracy: 25.58%\n",
      "Epoch 9/250 - Train Loss: 2.7532, Train Accuracy: 32.56% - Val Loss: 1.3960, Val Accuracy: 24.42%\n",
      "Epoch 10/250 - Train Loss: 2.7496, Train Accuracy: 33.42% - Val Loss: 1.3958, Val Accuracy: 27.25%\n",
      "Saved best models with validation accuracy: 27.2500\n",
      "Epoch 11/250 - Train Loss: 2.7478, Train Accuracy: 34.06% - Val Loss: 1.3964, Val Accuracy: 27.08%\n",
      "Epoch 12/250 - Train Loss: 2.7409, Train Accuracy: 35.40% - Val Loss: 1.3977, Val Accuracy: 28.50%\n",
      "Saved best models with validation accuracy: 28.5000\n",
      "Epoch 13/250 - Train Loss: 2.7356, Train Accuracy: 35.50% - Val Loss: 1.3914, Val Accuracy: 28.25%\n",
      "Epoch 14/250 - Train Loss: 2.7279, Train Accuracy: 36.90% - Val Loss: 1.3925, Val Accuracy: 28.75%\n",
      "Saved best models with validation accuracy: 28.7500\n",
      "Epoch 15/250 - Train Loss: 2.7192, Train Accuracy: 38.33% - Val Loss: 1.3834, Val Accuracy: 32.08%\n",
      "Saved best models with validation accuracy: 32.0833\n",
      "Epoch 16/250 - Train Loss: 2.7134, Train Accuracy: 39.10% - Val Loss: 1.3723, Val Accuracy: 34.17%\n",
      "Saved best models with validation accuracy: 34.1667\n",
      "Epoch 17/250 - Train Loss: 2.7092, Train Accuracy: 39.33% - Val Loss: 1.3702, Val Accuracy: 34.92%\n",
      "Saved best models with validation accuracy: 34.9167\n",
      "Epoch 18/250 - Train Loss: 2.7019, Train Accuracy: 38.83% - Val Loss: 1.3713, Val Accuracy: 33.92%\n",
      "Epoch 19/250 - Train Loss: 2.6906, Train Accuracy: 40.40% - Val Loss: 1.3588, Val Accuracy: 36.17%\n",
      "Saved best models with validation accuracy: 36.1667\n",
      "Epoch 20/250 - Train Loss: 2.6770, Train Accuracy: 40.50% - Val Loss: 1.3427, Val Accuracy: 37.92%\n",
      "Saved best models with validation accuracy: 37.9167\n",
      "Epoch 21/250 - Train Loss: 2.6699, Train Accuracy: 41.35% - Val Loss: 1.3418, Val Accuracy: 35.33%\n",
      "Epoch 22/250 - Train Loss: 2.6588, Train Accuracy: 42.42% - Val Loss: 1.3192, Val Accuracy: 40.33%\n",
      "Saved best models with validation accuracy: 40.3333\n",
      "Epoch 23/250 - Train Loss: 2.6598, Train Accuracy: 42.81% - Val Loss: 1.3186, Val Accuracy: 38.75%\n",
      "Epoch 24/250 - Train Loss: 2.6399, Train Accuracy: 42.42% - Val Loss: 1.3104, Val Accuracy: 40.92%\n",
      "Saved best models with validation accuracy: 40.9167\n",
      "Epoch 25/250 - Train Loss: 2.6405, Train Accuracy: 43.38% - Val Loss: 1.2839, Val Accuracy: 44.08%\n",
      "Saved best models with validation accuracy: 44.0833\n",
      "Epoch 26/250 - Train Loss: 2.6152, Train Accuracy: 45.27% - Val Loss: 1.2894, Val Accuracy: 43.58%\n",
      "Epoch 27/250 - Train Loss: 2.6061, Train Accuracy: 45.38% - Val Loss: 1.2879, Val Accuracy: 42.17%\n",
      "Epoch 28/250 - Train Loss: 2.6131, Train Accuracy: 45.19% - Val Loss: 1.2882, Val Accuracy: 43.08%\n",
      "Epoch 29/250 - Train Loss: 2.6006, Train Accuracy: 46.54% - Val Loss: 1.2659, Val Accuracy: 47.00%\n",
      "Saved best models with validation accuracy: 47.0000\n",
      "Epoch 30/250 - Train Loss: 2.5907, Train Accuracy: 46.75% - Val Loss: 1.2647, Val Accuracy: 45.83%\n",
      "Epoch 31/250 - Train Loss: 2.5797, Train Accuracy: 48.02% - Val Loss: 1.2653, Val Accuracy: 45.08%\n",
      "Epoch 32/250 - Train Loss: 2.5714, Train Accuracy: 47.96% - Val Loss: 1.2410, Val Accuracy: 47.58%\n",
      "Saved best models with validation accuracy: 47.5833\n",
      "Epoch 33/250 - Train Loss: 2.5539, Train Accuracy: 48.75% - Val Loss: 1.2358, Val Accuracy: 47.17%\n",
      "Epoch 34/250 - Train Loss: 2.5454, Train Accuracy: 49.38% - Val Loss: 1.2344, Val Accuracy: 50.75%\n",
      "Saved best models with validation accuracy: 50.7500\n",
      "Epoch 35/250 - Train Loss: 2.5483, Train Accuracy: 49.48% - Val Loss: 1.2225, Val Accuracy: 49.42%\n",
      "Epoch 36/250 - Train Loss: 2.5364, Train Accuracy: 49.31% - Val Loss: 1.2285, Val Accuracy: 49.25%\n",
      "Epoch 37/250 - Train Loss: 2.5223, Train Accuracy: 50.19% - Val Loss: 1.2137, Val Accuracy: 50.83%\n",
      "Saved best models with validation accuracy: 50.8333\n",
      "Epoch 38/250 - Train Loss: 2.4968, Train Accuracy: 52.06% - Val Loss: 1.2040, Val Accuracy: 52.92%\n",
      "Saved best models with validation accuracy: 52.9167\n",
      "Epoch 39/250 - Train Loss: 2.5059, Train Accuracy: 51.00% - Val Loss: 1.2056, Val Accuracy: 52.25%\n",
      "Epoch 40/250 - Train Loss: 2.4832, Train Accuracy: 53.96% - Val Loss: 1.2042, Val Accuracy: 49.83%\n",
      "Epoch 41/250 - Train Loss: 2.4784, Train Accuracy: 53.12% - Val Loss: 1.1810, Val Accuracy: 52.83%\n",
      "Epoch 42/250 - Train Loss: 2.4823, Train Accuracy: 52.58% - Val Loss: 1.1712, Val Accuracy: 54.00%\n",
      "Saved best models with validation accuracy: 54.0000\n",
      "Epoch 43/250 - Train Loss: 2.4745, Train Accuracy: 53.27% - Val Loss: 1.1839, Val Accuracy: 52.83%\n",
      "Epoch 44/250 - Train Loss: 2.4586, Train Accuracy: 53.94% - Val Loss: 1.1764, Val Accuracy: 55.25%\n",
      "Saved best models with validation accuracy: 55.2500\n",
      "Epoch 45/250 - Train Loss: 2.4282, Train Accuracy: 55.12% - Val Loss: 1.1548, Val Accuracy: 57.83%\n",
      "Saved best models with validation accuracy: 57.8333\n",
      "Epoch 46/250 - Train Loss: 2.4331, Train Accuracy: 55.58% - Val Loss: 1.1517, Val Accuracy: 56.33%\n",
      "Epoch 47/250 - Train Loss: 2.4180, Train Accuracy: 55.96% - Val Loss: 1.1333, Val Accuracy: 58.50%\n",
      "Saved best models with validation accuracy: 58.5000\n",
      "Epoch 48/250 - Train Loss: 2.4110, Train Accuracy: 56.48% - Val Loss: 1.1297, Val Accuracy: 58.75%\n",
      "Saved best models with validation accuracy: 58.7500\n",
      "Epoch 49/250 - Train Loss: 2.4021, Train Accuracy: 57.81% - Val Loss: 1.1323, Val Accuracy: 59.00%\n",
      "Saved best models with validation accuracy: 59.0000\n",
      "Epoch 50/250 - Train Loss: 2.3982, Train Accuracy: 57.12% - Val Loss: 1.1266, Val Accuracy: 59.75%\n",
      "Saved best models with validation accuracy: 59.7500\n",
      "Epoch 51/250 - Train Loss: 2.3817, Train Accuracy: 57.77% - Val Loss: 1.1199, Val Accuracy: 58.50%\n",
      "Epoch 52/250 - Train Loss: 2.3694, Train Accuracy: 58.31% - Val Loss: 1.1013, Val Accuracy: 61.58%\n",
      "Saved best models with validation accuracy: 61.5833\n",
      "Epoch 53/250 - Train Loss: 2.3722, Train Accuracy: 58.94% - Val Loss: 1.1257, Val Accuracy: 59.58%\n",
      "Epoch 54/250 - Train Loss: 2.3464, Train Accuracy: 60.19% - Val Loss: 1.1045, Val Accuracy: 60.75%\n",
      "Epoch 55/250 - Train Loss: 2.3406, Train Accuracy: 60.02% - Val Loss: 1.1036, Val Accuracy: 61.17%\n",
      "Epoch 56/250 - Train Loss: 2.3369, Train Accuracy: 60.23% - Val Loss: 1.0929, Val Accuracy: 61.83%\n",
      "Saved best models with validation accuracy: 61.8333\n",
      "Epoch 57/250 - Train Loss: 2.3189, Train Accuracy: 60.92% - Val Loss: 1.0963, Val Accuracy: 62.25%\n",
      "Saved best models with validation accuracy: 62.2500\n",
      "Epoch 58/250 - Train Loss: 2.3050, Train Accuracy: 61.81% - Val Loss: 1.0758, Val Accuracy: 62.42%\n",
      "Saved best models with validation accuracy: 62.4167\n",
      "Epoch 59/250 - Train Loss: 2.2814, Train Accuracy: 62.21% - Val Loss: 1.0727, Val Accuracy: 63.75%\n",
      "Saved best models with validation accuracy: 63.7500\n",
      "Epoch 60/250 - Train Loss: 2.2697, Train Accuracy: 63.85% - Val Loss: 1.0875, Val Accuracy: 62.42%\n",
      "Epoch 61/250 - Train Loss: 2.2477, Train Accuracy: 64.88% - Val Loss: 1.0696, Val Accuracy: 63.75%\n",
      "Epoch 62/250 - Train Loss: 2.2626, Train Accuracy: 64.17% - Val Loss: 1.0704, Val Accuracy: 63.25%\n",
      "Epoch 63/250 - Train Loss: 2.2550, Train Accuracy: 64.23% - Val Loss: 1.0368, Val Accuracy: 65.92%\n",
      "Saved best models with validation accuracy: 65.9167\n",
      "Epoch 64/250 - Train Loss: 2.2343, Train Accuracy: 65.31% - Val Loss: 1.0256, Val Accuracy: 68.17%\n",
      "Saved best models with validation accuracy: 68.1667\n",
      "Epoch 65/250 - Train Loss: 2.2390, Train Accuracy: 65.10% - Val Loss: 1.0542, Val Accuracy: 64.83%\n",
      "Epoch 66/250 - Train Loss: 2.2250, Train Accuracy: 65.50% - Val Loss: 1.0349, Val Accuracy: 68.25%\n",
      "Saved best models with validation accuracy: 68.2500\n",
      "Epoch 67/250 - Train Loss: 2.2113, Train Accuracy: 66.56% - Val Loss: 1.0137, Val Accuracy: 69.42%\n",
      "Saved best models with validation accuracy: 69.4167\n",
      "Epoch 68/250 - Train Loss: 2.2081, Train Accuracy: 66.21% - Val Loss: 1.0378, Val Accuracy: 67.50%\n",
      "Epoch 69/250 - Train Loss: 2.2022, Train Accuracy: 66.90% - Val Loss: 1.0387, Val Accuracy: 66.42%\n",
      "Epoch 70/250 - Train Loss: 2.1766, Train Accuracy: 68.38% - Val Loss: 0.9905, Val Accuracy: 70.17%\n",
      "Saved best models with validation accuracy: 70.1667\n",
      "Epoch 71/250 - Train Loss: 2.1837, Train Accuracy: 67.58% - Val Loss: 1.0009, Val Accuracy: 69.83%\n",
      "Epoch 72/250 - Train Loss: 2.1368, Train Accuracy: 69.40% - Val Loss: 1.0000, Val Accuracy: 71.17%\n",
      "Saved best models with validation accuracy: 71.1667\n",
      "Epoch 73/250 - Train Loss: 2.1477, Train Accuracy: 69.29% - Val Loss: 0.9942, Val Accuracy: 71.42%\n",
      "Saved best models with validation accuracy: 71.4167\n",
      "Epoch 74/250 - Train Loss: 2.1303, Train Accuracy: 70.08% - Val Loss: 0.9809, Val Accuracy: 71.67%\n",
      "Saved best models with validation accuracy: 71.6667\n",
      "Epoch 75/250 - Train Loss: 2.1187, Train Accuracy: 70.56% - Val Loss: 0.9873, Val Accuracy: 71.50%\n",
      "Epoch 76/250 - Train Loss: 2.1009, Train Accuracy: 70.65% - Val Loss: 0.9682, Val Accuracy: 73.33%\n",
      "Saved best models with validation accuracy: 73.3333\n",
      "Epoch 77/250 - Train Loss: 2.1119, Train Accuracy: 70.77% - Val Loss: 0.9533, Val Accuracy: 74.58%\n",
      "Saved best models with validation accuracy: 74.5833\n",
      "Epoch 78/250 - Train Loss: 2.0914, Train Accuracy: 72.10% - Val Loss: 0.9564, Val Accuracy: 73.92%\n",
      "Epoch 79/250 - Train Loss: 2.0514, Train Accuracy: 73.35% - Val Loss: 0.9501, Val Accuracy: 74.17%\n",
      "Epoch 80/250 - Train Loss: 2.0589, Train Accuracy: 73.38% - Val Loss: 0.9433, Val Accuracy: 75.00%\n",
      "Saved best models with validation accuracy: 75.0000\n",
      "Epoch 81/250 - Train Loss: 2.0429, Train Accuracy: 73.67% - Val Loss: 0.9725, Val Accuracy: 72.33%\n",
      "Epoch 82/250 - Train Loss: 2.0333, Train Accuracy: 73.73% - Val Loss: 0.9350, Val Accuracy: 75.17%\n",
      "Saved best models with validation accuracy: 75.1667\n",
      "Epoch 83/250 - Train Loss: 2.0048, Train Accuracy: 75.69% - Val Loss: 0.9178, Val Accuracy: 76.92%\n",
      "Saved best models with validation accuracy: 76.9167\n",
      "Epoch 84/250 - Train Loss: 2.0105, Train Accuracy: 74.81% - Val Loss: 0.9184, Val Accuracy: 76.50%\n",
      "Epoch 85/250 - Train Loss: 1.9921, Train Accuracy: 76.56% - Val Loss: 0.9366, Val Accuracy: 75.00%\n",
      "Epoch 86/250 - Train Loss: 2.0184, Train Accuracy: 75.25% - Val Loss: 0.9147, Val Accuracy: 76.42%\n",
      "Epoch 87/250 - Train Loss: 1.9882, Train Accuracy: 75.85% - Val Loss: 0.9210, Val Accuracy: 76.67%\n",
      "Epoch 88/250 - Train Loss: 1.9929, Train Accuracy: 76.23% - Val Loss: 0.9005, Val Accuracy: 78.33%\n",
      "Saved best models with validation accuracy: 78.3333\n",
      "Epoch 89/250 - Train Loss: 1.9688, Train Accuracy: 77.10% - Val Loss: 0.9041, Val Accuracy: 78.17%\n",
      "Epoch 90/250 - Train Loss: 1.9578, Train Accuracy: 77.88% - Val Loss: 0.9021, Val Accuracy: 77.42%\n",
      "Epoch 91/250 - Train Loss: 1.9661, Train Accuracy: 77.10% - Val Loss: 0.8953, Val Accuracy: 78.92%\n",
      "Saved best models with validation accuracy: 78.9167\n",
      "Epoch 92/250 - Train Loss: 1.9320, Train Accuracy: 78.31% - Val Loss: 0.8969, Val Accuracy: 78.17%\n",
      "Epoch 93/250 - Train Loss: 1.9421, Train Accuracy: 77.56% - Val Loss: 0.8892, Val Accuracy: 77.50%\n",
      "Epoch 94/250 - Train Loss: 1.9351, Train Accuracy: 78.08% - Val Loss: 0.8650, Val Accuracy: 80.08%\n",
      "Saved best models with validation accuracy: 80.0833\n",
      "Epoch 95/250 - Train Loss: 1.9237, Train Accuracy: 78.44% - Val Loss: 0.8725, Val Accuracy: 80.25%\n",
      "Saved best models with validation accuracy: 80.2500\n",
      "Epoch 96/250 - Train Loss: 1.8909, Train Accuracy: 79.60% - Val Loss: 0.8505, Val Accuracy: 81.42%\n",
      "Saved best models with validation accuracy: 81.4167\n",
      "Epoch 97/250 - Train Loss: 1.9057, Train Accuracy: 79.00% - Val Loss: 0.8805, Val Accuracy: 80.08%\n",
      "Epoch 98/250 - Train Loss: 1.8814, Train Accuracy: 79.85% - Val Loss: 0.8584, Val Accuracy: 81.42%\n",
      "Epoch 99/250 - Train Loss: 1.8823, Train Accuracy: 79.96% - Val Loss: 0.8651, Val Accuracy: 81.33%\n",
      "Epoch 100/250 - Train Loss: 1.8589, Train Accuracy: 80.94% - Val Loss: 0.8543, Val Accuracy: 80.92%\n",
      "Epoch 101/250 - Train Loss: 1.8515, Train Accuracy: 81.08% - Val Loss: 0.8508, Val Accuracy: 82.25%\n",
      "Saved best models with validation accuracy: 82.2500\n",
      "Epoch 102/250 - Train Loss: 1.8329, Train Accuracy: 81.40% - Val Loss: 0.8573, Val Accuracy: 80.67%\n",
      "Epoch 103/250 - Train Loss: 1.8386, Train Accuracy: 81.73% - Val Loss: 0.8551, Val Accuracy: 82.42%\n",
      "Saved best models with validation accuracy: 82.4167\n",
      "Epoch 104/250 - Train Loss: 1.8345, Train Accuracy: 81.35% - Val Loss: 0.8242, Val Accuracy: 83.67%\n",
      "Saved best models with validation accuracy: 83.6667\n",
      "Epoch 105/250 - Train Loss: 1.8103, Train Accuracy: 82.96% - Val Loss: 0.8285, Val Accuracy: 83.00%\n",
      "Epoch 106/250 - Train Loss: 1.8013, Train Accuracy: 83.23% - Val Loss: 0.8345, Val Accuracy: 82.58%\n",
      "Epoch 107/250 - Train Loss: 1.7922, Train Accuracy: 82.98% - Val Loss: 0.8408, Val Accuracy: 82.33%\n",
      "Epoch 108/250 - Train Loss: 1.7998, Train Accuracy: 82.71% - Val Loss: 0.8287, Val Accuracy: 83.67%\n",
      "Epoch 109/250 - Train Loss: 1.7941, Train Accuracy: 83.31% - Val Loss: 0.8049, Val Accuracy: 84.67%\n",
      "Saved best models with validation accuracy: 84.6667\n",
      "Epoch 110/250 - Train Loss: 1.7727, Train Accuracy: 83.94% - Val Loss: 0.8197, Val Accuracy: 84.50%\n",
      "Epoch 111/250 - Train Loss: 1.7517, Train Accuracy: 84.56% - Val Loss: 0.8081, Val Accuracy: 85.42%\n",
      "Saved best models with validation accuracy: 85.4167\n",
      "Epoch 112/250 - Train Loss: 1.7776, Train Accuracy: 83.69% - Val Loss: 0.7928, Val Accuracy: 85.42%\n",
      "Epoch 113/250 - Train Loss: 1.7681, Train Accuracy: 83.65% - Val Loss: 0.8084, Val Accuracy: 84.25%\n",
      "Epoch 114/250 - Train Loss: 1.7608, Train Accuracy: 84.33% - Val Loss: 0.7913, Val Accuracy: 85.67%\n",
      "Saved best models with validation accuracy: 85.6667\n",
      "Epoch 115/250 - Train Loss: 1.7430, Train Accuracy: 84.23% - Val Loss: 0.7855, Val Accuracy: 86.67%\n",
      "Saved best models with validation accuracy: 86.6667\n",
      "Epoch 116/250 - Train Loss: 1.7451, Train Accuracy: 84.58% - Val Loss: 0.8108, Val Accuracy: 84.25%\n",
      "Epoch 117/250 - Train Loss: 1.7317, Train Accuracy: 85.31% - Val Loss: 0.7898, Val Accuracy: 86.08%\n",
      "Epoch 118/250 - Train Loss: 1.7451, Train Accuracy: 84.81% - Val Loss: 0.7774, Val Accuracy: 87.25%\n",
      "Saved best models with validation accuracy: 87.2500\n",
      "Epoch 119/250 - Train Loss: 1.7192, Train Accuracy: 85.56% - Val Loss: 0.7861, Val Accuracy: 87.42%\n",
      "Saved best models with validation accuracy: 87.4167\n",
      "Epoch 120/250 - Train Loss: 1.7115, Train Accuracy: 85.67% - Val Loss: 0.7659, Val Accuracy: 87.67%\n",
      "Saved best models with validation accuracy: 87.6667\n",
      "Epoch 121/250 - Train Loss: 1.7228, Train Accuracy: 84.96% - Val Loss: 0.7718, Val Accuracy: 86.25%\n",
      "Epoch 122/250 - Train Loss: 1.7026, Train Accuracy: 85.77% - Val Loss: 0.7895, Val Accuracy: 86.00%\n",
      "Epoch 123/250 - Train Loss: 1.6870, Train Accuracy: 86.75% - Val Loss: 0.7851, Val Accuracy: 85.83%\n",
      "Epoch 124/250 - Train Loss: 1.6937, Train Accuracy: 86.77% - Val Loss: 0.7632, Val Accuracy: 88.00%\n",
      "Saved best models with validation accuracy: 88.0000\n",
      "Epoch 125/250 - Train Loss: 1.6768, Train Accuracy: 86.96% - Val Loss: 0.7624, Val Accuracy: 88.25%\n",
      "Saved best models with validation accuracy: 88.2500\n",
      "Epoch 126/250 - Train Loss: 1.6778, Train Accuracy: 87.04% - Val Loss: 0.7641, Val Accuracy: 88.42%\n",
      "Saved best models with validation accuracy: 88.4167\n",
      "Epoch 127/250 - Train Loss: 1.6359, Train Accuracy: 88.27% - Val Loss: 0.7401, Val Accuracy: 89.92%\n",
      "Saved best models with validation accuracy: 89.9167\n",
      "Epoch 128/250 - Train Loss: 1.6595, Train Accuracy: 87.21% - Val Loss: 0.7637, Val Accuracy: 87.58%\n",
      "Epoch 129/250 - Train Loss: 1.6589, Train Accuracy: 87.81% - Val Loss: 0.7669, Val Accuracy: 87.25%\n",
      "Epoch 130/250 - Train Loss: 1.6743, Train Accuracy: 86.79% - Val Loss: 0.7565, Val Accuracy: 88.08%\n",
      "Epoch 131/250 - Train Loss: 1.6483, Train Accuracy: 88.29% - Val Loss: 0.7584, Val Accuracy: 88.42%\n",
      "Epoch 132/250 - Train Loss: 1.6261, Train Accuracy: 88.58% - Val Loss: 0.7571, Val Accuracy: 88.25%\n",
      "Epoch 133/250 - Train Loss: 1.6519, Train Accuracy: 87.69% - Val Loss: 0.7661, Val Accuracy: 87.25%\n",
      "Epoch 134/250 - Train Loss: 1.6277, Train Accuracy: 88.15% - Val Loss: 0.7488, Val Accuracy: 89.33%\n",
      "Early stopping triggered after 134 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "\n",
      "=== Making predictions on test data using ensemble of models ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24752\\1340472431.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_modelv1.pth'))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24752\\1340472431.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  auxiliarymodel1.load_state_dict(torch.load('best_modelv2.pth'))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_24752\\1340472431.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  auxiliarymodel2.load_state_dict(torch.load('best_modelv3.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from each model...\n",
      "Collected predictions from model 1\n",
      "Collected predictions from model 2\n",
      "Collected predictions from model 3\n",
      "Starting grid search for optimal weights...\n",
      "Searching through 501501 weight combinations...\n",
      "New best: Weights=[np.float64(0.0), np.float64(0.0), np.float64(1.0)], Accuracy=91.5000%\n",
      "New best: Weights=[np.float64(0.0), np.float64(0.038), np.float64(0.962)], Accuracy=92.0000%\n",
      "Processed 100/501501 combinations\n",
      "New best: Weights=[np.float64(0.0), np.float64(0.176), np.float64(0.824)], Accuracy=92.5000%\n",
      "Processed 200/501501 combinations\n",
      "New best: Weights=[np.float64(0.0), np.float64(0.27), np.float64(0.73)], Accuracy=93.0000%\n",
      "Processed 300/501501 combinations\n",
      "New best: Weights=[np.float64(0.0), np.float64(0.331), np.float64(0.669)], Accuracy=93.5000%\n",
      "New best: Weights=[np.float64(0.0), np.float64(0.34500000000000003), np.float64(0.655)], Accuracy=94.0000%\n",
      "New best: Weights=[np.float64(0.0), np.float64(0.37), np.float64(0.63)], Accuracy=94.5000%\n",
      "Processed 400/501501 combinations\n",
      "New best: Weights=[np.float64(0.0), np.float64(0.43), np.float64(0.57)], Accuracy=95.0000%\n",
      "New best: Weights=[np.float64(0.0), np.float64(0.443), np.float64(0.557)], Accuracy=95.5000%\n",
      "Processed 500/501501 combinations\n",
      "Processed 600/501501 combinations\n",
      "Processed 700/501501 combinations\n",
      "Processed 800/501501 combinations\n",
      "Processed 900/501501 combinations\n",
      "Processed 1000/501501 combinations\n",
      "Processed 1100/501501 combinations\n",
      "Processed 1200/501501 combinations\n",
      "Processed 1300/501501 combinations\n",
      "Processed 1400/501501 combinations\n",
      "Processed 1500/501501 combinations\n",
      "Processed 1600/501501 combinations\n",
      "Processed 1700/501501 combinations\n",
      "Processed 1800/501501 combinations\n",
      "Processed 1900/501501 combinations\n",
      "Processed 2000/501501 combinations\n",
      "Processed 2100/501501 combinations\n",
      "Processed 2200/501501 combinations\n",
      "Processed 2300/501501 combinations\n",
      "Processed 2400/501501 combinations\n",
      "Processed 2500/501501 combinations\n",
      "Processed 2600/501501 combinations\n",
      "Processed 2700/501501 combinations\n",
      "Processed 2800/501501 combinations\n",
      "Processed 2900/501501 combinations\n",
      "Processed 3000/501501 combinations\n",
      "Processed 3100/501501 combinations\n",
      "Processed 3200/501501 combinations\n",
      "Processed 3300/501501 combinations\n",
      "Processed 3400/501501 combinations\n",
      "Processed 3500/501501 combinations\n",
      "Processed 3600/501501 combinations\n",
      "Processed 3700/501501 combinations\n",
      "Processed 3800/501501 combinations\n",
      "Processed 3900/501501 combinations\n",
      "Processed 4000/501501 combinations\n",
      "Processed 4100/501501 combinations\n",
      "Processed 4200/501501 combinations\n",
      "Processed 4300/501501 combinations\n",
      "Processed 4400/501501 combinations\n",
      "Processed 4500/501501 combinations\n",
      "Processed 4600/501501 combinations\n",
      "Processed 4700/501501 combinations\n",
      "Processed 4800/501501 combinations\n",
      "Processed 4900/501501 combinations\n",
      "Processed 5000/501501 combinations\n",
      "Processed 5100/501501 combinations\n",
      "Processed 5200/501501 combinations\n",
      "Processed 5300/501501 combinations\n",
      "Processed 5400/501501 combinations\n",
      "Processed 5500/501501 combinations\n",
      "Processed 5600/501501 combinations\n",
      "Processed 5700/501501 combinations\n",
      "Processed 5800/501501 combinations\n",
      "Processed 5900/501501 combinations\n",
      "Processed 6000/501501 combinations\n",
      "Processed 6100/501501 combinations\n",
      "Processed 6200/501501 combinations\n",
      "Processed 6300/501501 combinations\n",
      "Processed 6400/501501 combinations\n",
      "Processed 6500/501501 combinations\n",
      "Processed 6600/501501 combinations\n",
      "Processed 6700/501501 combinations\n",
      "Processed 6800/501501 combinations\n",
      "Processed 6900/501501 combinations\n",
      "Processed 7000/501501 combinations\n",
      "Processed 7100/501501 combinations\n",
      "Processed 7200/501501 combinations\n",
      "Processed 7300/501501 combinations\n",
      "Processed 7400/501501 combinations\n",
      "Processed 7500/501501 combinations\n",
      "Processed 7600/501501 combinations\n",
      "Processed 7700/501501 combinations\n",
      "Processed 7800/501501 combinations\n",
      "Processed 7900/501501 combinations\n",
      "Processed 8000/501501 combinations\n",
      "Processed 8100/501501 combinations\n",
      "Processed 8200/501501 combinations\n",
      "Processed 8300/501501 combinations\n",
      "Processed 8400/501501 combinations\n",
      "Processed 8500/501501 combinations\n",
      "Processed 8600/501501 combinations\n",
      "Processed 8700/501501 combinations\n",
      "Processed 8800/501501 combinations\n",
      "Processed 8900/501501 combinations\n",
      "Processed 9000/501501 combinations\n",
      "Processed 9100/501501 combinations\n",
      "Processed 9200/501501 combinations\n",
      "Processed 9300/501501 combinations\n",
      "Processed 9400/501501 combinations\n",
      "Processed 9500/501501 combinations\n",
      "Processed 9600/501501 combinations\n",
      "Processed 9700/501501 combinations\n",
      "Processed 9800/501501 combinations\n",
      "Processed 9900/501501 combinations\n",
      "Processed 10000/501501 combinations\n",
      "Processed 10100/501501 combinations\n",
      "Processed 10200/501501 combinations\n",
      "Processed 10300/501501 combinations\n",
      "Processed 10400/501501 combinations\n",
      "Processed 10500/501501 combinations\n",
      "Processed 10600/501501 combinations\n",
      "Processed 10700/501501 combinations\n",
      "Processed 10800/501501 combinations\n",
      "Processed 10900/501501 combinations\n",
      "Processed 11000/501501 combinations\n",
      "Processed 11100/501501 combinations\n",
      "Processed 11200/501501 combinations\n",
      "Processed 11300/501501 combinations\n",
      "Processed 11400/501501 combinations\n",
      "Processed 11500/501501 combinations\n",
      "Processed 11600/501501 combinations\n",
      "Processed 11700/501501 combinations\n",
      "Processed 11800/501501 combinations\n",
      "Processed 11900/501501 combinations\n",
      "Processed 12000/501501 combinations\n",
      "Processed 12100/501501 combinations\n",
      "Processed 12200/501501 combinations\n",
      "Processed 12300/501501 combinations\n",
      "Processed 12400/501501 combinations\n",
      "Processed 12500/501501 combinations\n",
      "Processed 12600/501501 combinations\n",
      "Processed 12700/501501 combinations\n",
      "Processed 12800/501501 combinations\n",
      "Processed 12900/501501 combinations\n",
      "Processed 13000/501501 combinations\n",
      "Processed 13100/501501 combinations\n",
      "Processed 13200/501501 combinations\n",
      "Processed 13300/501501 combinations\n",
      "Processed 13400/501501 combinations\n",
      "Processed 13500/501501 combinations\n",
      "Processed 13600/501501 combinations\n",
      "Processed 13700/501501 combinations\n",
      "Processed 13800/501501 combinations\n",
      "Processed 13900/501501 combinations\n",
      "Processed 14000/501501 combinations\n",
      "Processed 14100/501501 combinations\n",
      "Processed 14200/501501 combinations\n",
      "Processed 14300/501501 combinations\n",
      "Processed 14400/501501 combinations\n",
      "Processed 14500/501501 combinations\n",
      "Processed 14600/501501 combinations\n",
      "Processed 14700/501501 combinations\n",
      "Processed 14800/501501 combinations\n",
      "Processed 14900/501501 combinations\n",
      "Processed 15000/501501 combinations\n",
      "Processed 15100/501501 combinations\n",
      "Processed 15200/501501 combinations\n",
      "Processed 15300/501501 combinations\n",
      "Processed 15400/501501 combinations\n",
      "Processed 15500/501501 combinations\n",
      "Processed 15600/501501 combinations\n",
      "Processed 15700/501501 combinations\n",
      "Processed 15800/501501 combinations\n",
      "Processed 15900/501501 combinations\n",
      "Processed 16000/501501 combinations\n",
      "Processed 16100/501501 combinations\n",
      "Processed 16200/501501 combinations\n",
      "Processed 16300/501501 combinations\n",
      "Processed 16400/501501 combinations\n",
      "Processed 16500/501501 combinations\n",
      "Processed 16600/501501 combinations\n",
      "Processed 16700/501501 combinations\n",
      "Processed 16800/501501 combinations\n",
      "Processed 16900/501501 combinations\n",
      "Processed 17000/501501 combinations\n",
      "Processed 17100/501501 combinations\n",
      "Processed 17200/501501 combinations\n",
      "Processed 17300/501501 combinations\n",
      "Processed 17400/501501 combinations\n",
      "Processed 17500/501501 combinations\n",
      "Processed 17600/501501 combinations\n",
      "Processed 17700/501501 combinations\n",
      "Processed 17800/501501 combinations\n",
      "Processed 17900/501501 combinations\n",
      "Processed 18000/501501 combinations\n",
      "Processed 18100/501501 combinations\n",
      "Processed 18200/501501 combinations\n",
      "Processed 18300/501501 combinations\n",
      "Processed 18400/501501 combinations\n",
      "Processed 18500/501501 combinations\n",
      "Processed 18600/501501 combinations\n",
      "Processed 18700/501501 combinations\n",
      "Processed 18800/501501 combinations\n",
      "Processed 18900/501501 combinations\n",
      "Processed 19000/501501 combinations\n",
      "Processed 19100/501501 combinations\n",
      "Processed 19200/501501 combinations\n",
      "Processed 19300/501501 combinations\n",
      "Processed 19400/501501 combinations\n",
      "Processed 19500/501501 combinations\n",
      "Processed 19600/501501 combinations\n",
      "Processed 19700/501501 combinations\n",
      "Processed 19800/501501 combinations\n",
      "Processed 19900/501501 combinations\n",
      "Processed 20000/501501 combinations\n",
      "Processed 20100/501501 combinations\n",
      "Processed 20200/501501 combinations\n",
      "Processed 20300/501501 combinations\n",
      "Processed 20400/501501 combinations\n",
      "Processed 20500/501501 combinations\n",
      "Processed 20600/501501 combinations\n",
      "Processed 20700/501501 combinations\n",
      "Processed 20800/501501 combinations\n",
      "Processed 20900/501501 combinations\n",
      "Processed 21000/501501 combinations\n",
      "Processed 21100/501501 combinations\n",
      "Processed 21200/501501 combinations\n",
      "Processed 21300/501501 combinations\n",
      "Processed 21400/501501 combinations\n",
      "Processed 21500/501501 combinations\n",
      "Processed 21600/501501 combinations\n",
      "Processed 21700/501501 combinations\n",
      "Processed 21800/501501 combinations\n",
      "Processed 21900/501501 combinations\n",
      "Processed 22000/501501 combinations\n",
      "Processed 22100/501501 combinations\n",
      "Processed 22200/501501 combinations\n",
      "Processed 22300/501501 combinations\n",
      "Processed 22400/501501 combinations\n",
      "Processed 22500/501501 combinations\n",
      "Processed 22600/501501 combinations\n",
      "Processed 22700/501501 combinations\n",
      "Processed 22800/501501 combinations\n",
      "Processed 22900/501501 combinations\n",
      "Processed 23000/501501 combinations\n",
      "Processed 23100/501501 combinations\n",
      "Processed 23200/501501 combinations\n",
      "Processed 23300/501501 combinations\n",
      "Processed 23400/501501 combinations\n",
      "Processed 23500/501501 combinations\n",
      "Processed 23600/501501 combinations\n",
      "Processed 23700/501501 combinations\n",
      "Processed 23800/501501 combinations\n",
      "Processed 23900/501501 combinations\n",
      "Processed 24000/501501 combinations\n",
      "Processed 24100/501501 combinations\n",
      "Processed 24200/501501 combinations\n",
      "Processed 24300/501501 combinations\n",
      "Processed 24400/501501 combinations\n",
      "Processed 24500/501501 combinations\n",
      "Processed 24600/501501 combinations\n",
      "Processed 24700/501501 combinations\n",
      "Processed 24800/501501 combinations\n",
      "Processed 24900/501501 combinations\n",
      "Processed 25000/501501 combinations\n",
      "Processed 25100/501501 combinations\n",
      "Processed 25200/501501 combinations\n",
      "Processed 25300/501501 combinations\n",
      "Processed 25400/501501 combinations\n",
      "Processed 25500/501501 combinations\n",
      "Processed 25600/501501 combinations\n",
      "Processed 25700/501501 combinations\n",
      "Processed 25800/501501 combinations\n",
      "Processed 25900/501501 combinations\n",
      "Processed 26000/501501 combinations\n",
      "Processed 26100/501501 combinations\n",
      "Processed 26200/501501 combinations\n",
      "Processed 26300/501501 combinations\n",
      "Processed 26400/501501 combinations\n",
      "Processed 26500/501501 combinations\n",
      "Processed 26600/501501 combinations\n",
      "Processed 26700/501501 combinations\n",
      "Processed 26800/501501 combinations\n",
      "Processed 26900/501501 combinations\n",
      "Processed 27000/501501 combinations\n",
      "Processed 27100/501501 combinations\n",
      "Processed 27200/501501 combinations\n",
      "Processed 27300/501501 combinations\n",
      "Processed 27400/501501 combinations\n",
      "Processed 27500/501501 combinations\n",
      "Processed 27600/501501 combinations\n",
      "Processed 27700/501501 combinations\n",
      "Processed 27800/501501 combinations\n",
      "Processed 27900/501501 combinations\n",
      "Processed 28000/501501 combinations\n",
      "Processed 28100/501501 combinations\n",
      "Processed 28200/501501 combinations\n",
      "Processed 28300/501501 combinations\n",
      "Processed 28400/501501 combinations\n",
      "Processed 28500/501501 combinations\n",
      "Processed 28600/501501 combinations\n",
      "Processed 28700/501501 combinations\n",
      "Processed 28800/501501 combinations\n",
      "Processed 28900/501501 combinations\n",
      "Processed 29000/501501 combinations\n",
      "Processed 29100/501501 combinations\n",
      "Processed 29200/501501 combinations\n",
      "Processed 29300/501501 combinations\n",
      "Processed 29400/501501 combinations\n",
      "Processed 29500/501501 combinations\n",
      "Processed 29600/501501 combinations\n",
      "Processed 29700/501501 combinations\n",
      "Processed 29800/501501 combinations\n",
      "Processed 29900/501501 combinations\n",
      "Processed 30000/501501 combinations\n",
      "Processed 30100/501501 combinations\n",
      "Processed 30200/501501 combinations\n",
      "Processed 30300/501501 combinations\n",
      "Processed 30400/501501 combinations\n",
      "Processed 30500/501501 combinations\n",
      "Processed 30600/501501 combinations\n",
      "Processed 30700/501501 combinations\n",
      "Processed 30800/501501 combinations\n",
      "Processed 30900/501501 combinations\n",
      "Processed 31000/501501 combinations\n",
      "Processed 31100/501501 combinations\n",
      "Processed 31200/501501 combinations\n",
      "Processed 31300/501501 combinations\n",
      "Processed 31400/501501 combinations\n",
      "Processed 31500/501501 combinations\n",
      "Processed 31600/501501 combinations\n",
      "Processed 31700/501501 combinations\n",
      "Processed 31800/501501 combinations\n",
      "Processed 31900/501501 combinations\n",
      "Processed 32000/501501 combinations\n",
      "Processed 32100/501501 combinations\n",
      "Processed 32200/501501 combinations\n",
      "Processed 32300/501501 combinations\n",
      "Processed 32400/501501 combinations\n",
      "Processed 32500/501501 combinations\n",
      "Processed 32600/501501 combinations\n",
      "Processed 32700/501501 combinations\n",
      "Processed 32800/501501 combinations\n",
      "Processed 32900/501501 combinations\n",
      "Processed 33000/501501 combinations\n",
      "Processed 33100/501501 combinations\n",
      "Processed 33200/501501 combinations\n",
      "Processed 33300/501501 combinations\n",
      "Processed 33400/501501 combinations\n",
      "Processed 33500/501501 combinations\n",
      "Processed 33600/501501 combinations\n",
      "Processed 33700/501501 combinations\n",
      "Processed 33800/501501 combinations\n",
      "Processed 33900/501501 combinations\n",
      "Processed 34000/501501 combinations\n",
      "Processed 34100/501501 combinations\n",
      "Processed 34200/501501 combinations\n",
      "Processed 34300/501501 combinations\n",
      "Processed 34400/501501 combinations\n",
      "Processed 34500/501501 combinations\n",
      "Processed 34600/501501 combinations\n",
      "Processed 34700/501501 combinations\n",
      "Processed 34800/501501 combinations\n",
      "Processed 34900/501501 combinations\n",
      "Processed 35000/501501 combinations\n",
      "Processed 35100/501501 combinations\n",
      "Processed 35200/501501 combinations\n",
      "Processed 35300/501501 combinations\n",
      "Processed 35400/501501 combinations\n",
      "Processed 35500/501501 combinations\n",
      "Processed 35600/501501 combinations\n",
      "Processed 35700/501501 combinations\n",
      "Processed 35800/501501 combinations\n",
      "Processed 35900/501501 combinations\n",
      "Processed 36000/501501 combinations\n",
      "Processed 36100/501501 combinations\n",
      "Processed 36200/501501 combinations\n",
      "Processed 36300/501501 combinations\n",
      "Processed 36400/501501 combinations\n",
      "Processed 36500/501501 combinations\n",
      "Processed 36600/501501 combinations\n",
      "Processed 36700/501501 combinations\n",
      "Processed 36800/501501 combinations\n",
      "Processed 36900/501501 combinations\n",
      "Processed 37000/501501 combinations\n",
      "Processed 37100/501501 combinations\n",
      "Processed 37200/501501 combinations\n",
      "Processed 37300/501501 combinations\n",
      "Processed 37400/501501 combinations\n",
      "Processed 37500/501501 combinations\n",
      "Processed 37600/501501 combinations\n",
      "Processed 37700/501501 combinations\n",
      "Processed 37800/501501 combinations\n",
      "Processed 37900/501501 combinations\n",
      "Processed 38000/501501 combinations\n",
      "Processed 38100/501501 combinations\n",
      "Processed 38200/501501 combinations\n",
      "Processed 38300/501501 combinations\n",
      "Processed 38400/501501 combinations\n",
      "Processed 38500/501501 combinations\n",
      "Processed 38600/501501 combinations\n",
      "Processed 38700/501501 combinations\n",
      "Processed 38800/501501 combinations\n",
      "Processed 38900/501501 combinations\n",
      "Processed 39000/501501 combinations\n",
      "Processed 39100/501501 combinations\n",
      "Processed 39200/501501 combinations\n",
      "Processed 39300/501501 combinations\n",
      "Processed 39400/501501 combinations\n",
      "Processed 39500/501501 combinations\n",
      "Processed 39600/501501 combinations\n",
      "Processed 39700/501501 combinations\n",
      "Processed 39800/501501 combinations\n",
      "Processed 39900/501501 combinations\n",
      "Processed 40000/501501 combinations\n",
      "Processed 40100/501501 combinations\n",
      "Processed 40200/501501 combinations\n",
      "Processed 40300/501501 combinations\n",
      "Processed 40400/501501 combinations\n",
      "Processed 40500/501501 combinations\n",
      "Processed 40600/501501 combinations\n",
      "Processed 40700/501501 combinations\n",
      "Processed 40800/501501 combinations\n",
      "Processed 40900/501501 combinations\n",
      "Processed 41000/501501 combinations\n",
      "Processed 41100/501501 combinations\n",
      "Processed 41200/501501 combinations\n",
      "Processed 41300/501501 combinations\n",
      "Processed 41400/501501 combinations\n",
      "Processed 41500/501501 combinations\n",
      "Processed 41600/501501 combinations\n",
      "Processed 41700/501501 combinations\n",
      "Processed 41800/501501 combinations\n",
      "Processed 41900/501501 combinations\n",
      "Processed 42000/501501 combinations\n",
      "Processed 42100/501501 combinations\n",
      "Processed 42200/501501 combinations\n",
      "Processed 42300/501501 combinations\n",
      "Processed 42400/501501 combinations\n",
      "Processed 42500/501501 combinations\n",
      "Processed 42600/501501 combinations\n",
      "Processed 42700/501501 combinations\n",
      "Processed 42800/501501 combinations\n",
      "Processed 42900/501501 combinations\n",
      "Processed 43000/501501 combinations\n",
      "Processed 43100/501501 combinations\n",
      "Processed 43200/501501 combinations\n",
      "Processed 43300/501501 combinations\n",
      "Processed 43400/501501 combinations\n",
      "Processed 43500/501501 combinations\n",
      "Processed 43600/501501 combinations\n",
      "Processed 43700/501501 combinations\n",
      "Processed 43800/501501 combinations\n",
      "Processed 43900/501501 combinations\n",
      "Processed 44000/501501 combinations\n",
      "Processed 44100/501501 combinations\n",
      "Processed 44200/501501 combinations\n",
      "Processed 44300/501501 combinations\n",
      "Processed 44400/501501 combinations\n",
      "Processed 44500/501501 combinations\n",
      "Processed 44600/501501 combinations\n",
      "Processed 44700/501501 combinations\n",
      "Processed 44800/501501 combinations\n",
      "Processed 44900/501501 combinations\n",
      "Processed 45000/501501 combinations\n",
      "Processed 45100/501501 combinations\n",
      "Processed 45200/501501 combinations\n",
      "Processed 45300/501501 combinations\n",
      "Processed 45400/501501 combinations\n",
      "Processed 45500/501501 combinations\n",
      "Processed 45600/501501 combinations\n",
      "Processed 45700/501501 combinations\n",
      "Processed 45800/501501 combinations\n",
      "Processed 45900/501501 combinations\n",
      "Processed 46000/501501 combinations\n",
      "Processed 46100/501501 combinations\n",
      "Processed 46200/501501 combinations\n",
      "Processed 46300/501501 combinations\n",
      "Processed 46400/501501 combinations\n",
      "Processed 46500/501501 combinations\n",
      "Processed 46600/501501 combinations\n",
      "Processed 46700/501501 combinations\n",
      "Processed 46800/501501 combinations\n",
      "Processed 46900/501501 combinations\n",
      "Processed 47000/501501 combinations\n",
      "Processed 47100/501501 combinations\n",
      "Processed 47200/501501 combinations\n",
      "Processed 47300/501501 combinations\n",
      "Processed 47400/501501 combinations\n",
      "Processed 47500/501501 combinations\n",
      "Processed 47600/501501 combinations\n",
      "Processed 47700/501501 combinations\n",
      "Processed 47800/501501 combinations\n",
      "Processed 47900/501501 combinations\n",
      "Processed 48000/501501 combinations\n",
      "Processed 48100/501501 combinations\n",
      "Processed 48200/501501 combinations\n",
      "Processed 48300/501501 combinations\n",
      "Processed 48400/501501 combinations\n",
      "Processed 48500/501501 combinations\n",
      "Processed 48600/501501 combinations\n",
      "Processed 48700/501501 combinations\n",
      "Processed 48800/501501 combinations\n",
      "Processed 48900/501501 combinations\n",
      "Processed 49000/501501 combinations\n",
      "Processed 49100/501501 combinations\n",
      "Processed 49200/501501 combinations\n",
      "Processed 49300/501501 combinations\n",
      "Processed 49400/501501 combinations\n",
      "Processed 49500/501501 combinations\n",
      "Processed 49600/501501 combinations\n",
      "Processed 49700/501501 combinations\n",
      "Processed 49800/501501 combinations\n",
      "Processed 49900/501501 combinations\n",
      "Processed 50000/501501 combinations\n",
      "Processed 50100/501501 combinations\n",
      "Processed 50200/501501 combinations\n",
      "Processed 50300/501501 combinations\n",
      "Processed 50400/501501 combinations\n",
      "Processed 50500/501501 combinations\n",
      "Processed 50600/501501 combinations\n",
      "Processed 50700/501501 combinations\n",
      "Processed 50800/501501 combinations\n",
      "Processed 50900/501501 combinations\n",
      "Processed 51000/501501 combinations\n",
      "Processed 51100/501501 combinations\n",
      "Processed 51200/501501 combinations\n",
      "Processed 51300/501501 combinations\n",
      "Processed 51400/501501 combinations\n",
      "Processed 51500/501501 combinations\n",
      "Processed 51600/501501 combinations\n",
      "Processed 51700/501501 combinations\n",
      "Processed 51800/501501 combinations\n",
      "Processed 51900/501501 combinations\n",
      "Processed 52000/501501 combinations\n",
      "Processed 52100/501501 combinations\n",
      "Processed 52200/501501 combinations\n",
      "Processed 52300/501501 combinations\n",
      "Processed 52400/501501 combinations\n",
      "Processed 52500/501501 combinations\n",
      "Processed 52600/501501 combinations\n",
      "Processed 52700/501501 combinations\n",
      "Processed 52800/501501 combinations\n",
      "Processed 52900/501501 combinations\n",
      "Processed 53000/501501 combinations\n",
      "Processed 53100/501501 combinations\n",
      "Processed 53200/501501 combinations\n",
      "Processed 53300/501501 combinations\n",
      "Processed 53400/501501 combinations\n",
      "Processed 53500/501501 combinations\n",
      "Processed 53600/501501 combinations\n",
      "Processed 53700/501501 combinations\n",
      "Processed 53800/501501 combinations\n",
      "Processed 53900/501501 combinations\n",
      "Processed 54000/501501 combinations\n",
      "Processed 54100/501501 combinations\n",
      "Processed 54200/501501 combinations\n",
      "Processed 54300/501501 combinations\n",
      "Processed 54400/501501 combinations\n",
      "Processed 54500/501501 combinations\n",
      "Processed 54600/501501 combinations\n",
      "Processed 54700/501501 combinations\n",
      "Processed 54800/501501 combinations\n",
      "Processed 54900/501501 combinations\n",
      "Processed 55000/501501 combinations\n",
      "Processed 55100/501501 combinations\n",
      "Processed 55200/501501 combinations\n",
      "Processed 55300/501501 combinations\n",
      "Processed 55400/501501 combinations\n",
      "Processed 55500/501501 combinations\n",
      "Processed 55600/501501 combinations\n",
      "Processed 55700/501501 combinations\n",
      "Processed 55800/501501 combinations\n",
      "Processed 55900/501501 combinations\n",
      "Processed 56000/501501 combinations\n",
      "Processed 56100/501501 combinations\n",
      "Processed 56200/501501 combinations\n",
      "Processed 56300/501501 combinations\n",
      "Processed 56400/501501 combinations\n",
      "Processed 56500/501501 combinations\n",
      "Processed 56600/501501 combinations\n",
      "Processed 56700/501501 combinations\n",
      "Processed 56800/501501 combinations\n",
      "Processed 56900/501501 combinations\n",
      "Processed 57000/501501 combinations\n",
      "Processed 57100/501501 combinations\n",
      "Processed 57200/501501 combinations\n",
      "Processed 57300/501501 combinations\n",
      "Processed 57400/501501 combinations\n",
      "Processed 57500/501501 combinations\n",
      "Processed 57600/501501 combinations\n",
      "Processed 57700/501501 combinations\n",
      "Processed 57800/501501 combinations\n",
      "Processed 57900/501501 combinations\n",
      "Processed 58000/501501 combinations\n",
      "Processed 58100/501501 combinations\n",
      "Processed 58200/501501 combinations\n",
      "Processed 58300/501501 combinations\n",
      "Processed 58400/501501 combinations\n",
      "Processed 58500/501501 combinations\n",
      "Processed 58600/501501 combinations\n",
      "Processed 58700/501501 combinations\n",
      "Processed 58800/501501 combinations\n",
      "Processed 58900/501501 combinations\n",
      "Processed 59000/501501 combinations\n",
      "Processed 59100/501501 combinations\n",
      "Processed 59200/501501 combinations\n",
      "Processed 59300/501501 combinations\n",
      "Processed 59400/501501 combinations\n",
      "Processed 59500/501501 combinations\n",
      "Processed 59600/501501 combinations\n",
      "Processed 59700/501501 combinations\n",
      "Processed 59800/501501 combinations\n",
      "Processed 59900/501501 combinations\n",
      "Processed 60000/501501 combinations\n",
      "Processed 60100/501501 combinations\n",
      "Processed 60200/501501 combinations\n",
      "Processed 60300/501501 combinations\n",
      "Processed 60400/501501 combinations\n",
      "Processed 60500/501501 combinations\n",
      "Processed 60600/501501 combinations\n",
      "Processed 60700/501501 combinations\n",
      "Processed 60800/501501 combinations\n",
      "Processed 60900/501501 combinations\n",
      "Processed 61000/501501 combinations\n",
      "Processed 61100/501501 combinations\n",
      "Processed 61200/501501 combinations\n",
      "Processed 61300/501501 combinations\n",
      "Processed 61400/501501 combinations\n",
      "Processed 61500/501501 combinations\n",
      "Processed 61600/501501 combinations\n",
      "Processed 61700/501501 combinations\n",
      "Processed 61800/501501 combinations\n",
      "Processed 61900/501501 combinations\n",
      "Processed 62000/501501 combinations\n",
      "Processed 62100/501501 combinations\n",
      "Processed 62200/501501 combinations\n",
      "Processed 62300/501501 combinations\n",
      "Processed 62400/501501 combinations\n",
      "Processed 62500/501501 combinations\n",
      "Processed 62600/501501 combinations\n",
      "Processed 62700/501501 combinations\n",
      "Processed 62800/501501 combinations\n",
      "Processed 62900/501501 combinations\n",
      "Processed 63000/501501 combinations\n",
      "Processed 63100/501501 combinations\n",
      "Processed 63200/501501 combinations\n",
      "Processed 63300/501501 combinations\n",
      "Processed 63400/501501 combinations\n",
      "Processed 63500/501501 combinations\n",
      "Processed 63600/501501 combinations\n",
      "Processed 63700/501501 combinations\n",
      "Processed 63800/501501 combinations\n",
      "Processed 63900/501501 combinations\n",
      "Processed 64000/501501 combinations\n",
      "Processed 64100/501501 combinations\n",
      "Processed 64200/501501 combinations\n",
      "Processed 64300/501501 combinations\n",
      "Processed 64400/501501 combinations\n",
      "Processed 64500/501501 combinations\n",
      "Processed 64600/501501 combinations\n",
      "Processed 64700/501501 combinations\n",
      "Processed 64800/501501 combinations\n",
      "Processed 64900/501501 combinations\n",
      "Processed 65000/501501 combinations\n",
      "Processed 65100/501501 combinations\n",
      "Processed 65200/501501 combinations\n",
      "Processed 65300/501501 combinations\n",
      "Processed 65400/501501 combinations\n",
      "Processed 65500/501501 combinations\n",
      "Processed 65600/501501 combinations\n",
      "Processed 65700/501501 combinations\n",
      "Processed 65800/501501 combinations\n",
      "Processed 65900/501501 combinations\n",
      "Processed 66000/501501 combinations\n",
      "Processed 66100/501501 combinations\n",
      "Processed 66200/501501 combinations\n",
      "Processed 66300/501501 combinations\n",
      "Processed 66400/501501 combinations\n",
      "Processed 66500/501501 combinations\n",
      "Processed 66600/501501 combinations\n",
      "Processed 66700/501501 combinations\n",
      "Processed 66800/501501 combinations\n",
      "Processed 66900/501501 combinations\n",
      "Processed 67000/501501 combinations\n",
      "Processed 67100/501501 combinations\n",
      "Processed 67200/501501 combinations\n",
      "Processed 67300/501501 combinations\n",
      "Processed 67400/501501 combinations\n",
      "Processed 67500/501501 combinations\n",
      "Processed 67600/501501 combinations\n",
      "Processed 67700/501501 combinations\n",
      "Processed 67800/501501 combinations\n",
      "Processed 67900/501501 combinations\n",
      "Processed 68000/501501 combinations\n",
      "Processed 68100/501501 combinations\n",
      "Processed 68200/501501 combinations\n",
      "Processed 68300/501501 combinations\n",
      "Processed 68400/501501 combinations\n",
      "Processed 68500/501501 combinations\n",
      "Processed 68600/501501 combinations\n",
      "Processed 68700/501501 combinations\n",
      "Processed 68800/501501 combinations\n",
      "Processed 68900/501501 combinations\n",
      "Processed 69000/501501 combinations\n",
      "Processed 69100/501501 combinations\n",
      "Processed 69200/501501 combinations\n",
      "Processed 69300/501501 combinations\n",
      "Processed 69400/501501 combinations\n",
      "Processed 69500/501501 combinations\n",
      "Processed 69600/501501 combinations\n",
      "Processed 69700/501501 combinations\n",
      "Processed 69800/501501 combinations\n",
      "Processed 69900/501501 combinations\n",
      "Processed 70000/501501 combinations\n",
      "Processed 70100/501501 combinations\n",
      "Processed 70200/501501 combinations\n",
      "Processed 70300/501501 combinations\n",
      "Processed 70400/501501 combinations\n",
      "Processed 70500/501501 combinations\n",
      "Processed 70600/501501 combinations\n",
      "Processed 70700/501501 combinations\n",
      "Processed 70800/501501 combinations\n",
      "Processed 70900/501501 combinations\n",
      "Processed 71000/501501 combinations\n",
      "Processed 71100/501501 combinations\n",
      "Processed 71200/501501 combinations\n",
      "Processed 71300/501501 combinations\n",
      "Processed 71400/501501 combinations\n",
      "Processed 71500/501501 combinations\n",
      "Processed 71600/501501 combinations\n",
      "Processed 71700/501501 combinations\n",
      "Processed 71800/501501 combinations\n",
      "Processed 71900/501501 combinations\n",
      "Processed 72000/501501 combinations\n",
      "Processed 72100/501501 combinations\n",
      "Processed 72200/501501 combinations\n",
      "Processed 72300/501501 combinations\n",
      "Processed 72400/501501 combinations\n",
      "Processed 72500/501501 combinations\n",
      "Processed 72600/501501 combinations\n",
      "Processed 72700/501501 combinations\n",
      "Processed 72800/501501 combinations\n",
      "Processed 72900/501501 combinations\n",
      "Processed 73000/501501 combinations\n",
      "Processed 73100/501501 combinations\n",
      "Processed 73200/501501 combinations\n",
      "Processed 73300/501501 combinations\n",
      "Processed 73400/501501 combinations\n",
      "Processed 73500/501501 combinations\n",
      "Processed 73600/501501 combinations\n",
      "Processed 73700/501501 combinations\n",
      "Processed 73800/501501 combinations\n",
      "Processed 73900/501501 combinations\n",
      "Processed 74000/501501 combinations\n",
      "Processed 74100/501501 combinations\n",
      "Processed 74200/501501 combinations\n",
      "Processed 74300/501501 combinations\n",
      "Processed 74400/501501 combinations\n",
      "Processed 74500/501501 combinations\n",
      "Processed 74600/501501 combinations\n",
      "Processed 74700/501501 combinations\n",
      "Processed 74800/501501 combinations\n",
      "Processed 74900/501501 combinations\n",
      "Processed 75000/501501 combinations\n",
      "Processed 75100/501501 combinations\n",
      "Processed 75200/501501 combinations\n",
      "Processed 75300/501501 combinations\n",
      "Processed 75400/501501 combinations\n",
      "Processed 75500/501501 combinations\n",
      "Processed 75600/501501 combinations\n",
      "Processed 75700/501501 combinations\n",
      "Processed 75800/501501 combinations\n",
      "Processed 75900/501501 combinations\n",
      "Processed 76000/501501 combinations\n",
      "Processed 76100/501501 combinations\n",
      "Processed 76200/501501 combinations\n",
      "Processed 76300/501501 combinations\n",
      "Processed 76400/501501 combinations\n",
      "Processed 76500/501501 combinations\n",
      "Processed 76600/501501 combinations\n",
      "Processed 76700/501501 combinations\n",
      "Processed 76800/501501 combinations\n",
      "Processed 76900/501501 combinations\n",
      "Processed 77000/501501 combinations\n",
      "Processed 77100/501501 combinations\n",
      "Processed 77200/501501 combinations\n",
      "Processed 77300/501501 combinations\n",
      "Processed 77400/501501 combinations\n",
      "Processed 77500/501501 combinations\n",
      "Processed 77600/501501 combinations\n",
      "Processed 77700/501501 combinations\n",
      "Processed 77800/501501 combinations\n",
      "Processed 77900/501501 combinations\n",
      "Processed 78000/501501 combinations\n",
      "Processed 78100/501501 combinations\n",
      "Processed 78200/501501 combinations\n",
      "Processed 78300/501501 combinations\n",
      "Processed 78400/501501 combinations\n",
      "Processed 78500/501501 combinations\n",
      "Processed 78600/501501 combinations\n",
      "Processed 78700/501501 combinations\n",
      "Processed 78800/501501 combinations\n",
      "Processed 78900/501501 combinations\n",
      "Processed 79000/501501 combinations\n",
      "Processed 79100/501501 combinations\n",
      "Processed 79200/501501 combinations\n",
      "Processed 79300/501501 combinations\n",
      "Processed 79400/501501 combinations\n",
      "Processed 79500/501501 combinations\n",
      "Processed 79600/501501 combinations\n",
      "Processed 79700/501501 combinations\n",
      "Processed 79800/501501 combinations\n",
      "Processed 79900/501501 combinations\n",
      "Processed 80000/501501 combinations\n",
      "Processed 80100/501501 combinations\n",
      "New best: Weights=[np.float64(0.083), np.float64(0.47900000000000004), np.float64(0.438)], Accuracy=96.0000%\n",
      "Processed 80200/501501 combinations\n",
      "Processed 80300/501501 combinations\n",
      "Processed 80400/501501 combinations\n",
      "Processed 80500/501501 combinations\n",
      "Processed 80600/501501 combinations\n",
      "Processed 80700/501501 combinations\n",
      "Processed 80800/501501 combinations\n",
      "Processed 80900/501501 combinations\n",
      "Processed 81000/501501 combinations\n",
      "Processed 81100/501501 combinations\n",
      "Processed 81200/501501 combinations\n",
      "Processed 81300/501501 combinations\n",
      "Processed 81400/501501 combinations\n",
      "Processed 81500/501501 combinations\n",
      "Processed 81600/501501 combinations\n",
      "Processed 81700/501501 combinations\n",
      "Processed 81800/501501 combinations\n",
      "Processed 81900/501501 combinations\n",
      "Processed 82000/501501 combinations\n",
      "Processed 82100/501501 combinations\n",
      "Processed 82200/501501 combinations\n",
      "Processed 82300/501501 combinations\n",
      "Processed 82400/501501 combinations\n",
      "Processed 82500/501501 combinations\n",
      "Processed 82600/501501 combinations\n",
      "Processed 82700/501501 combinations\n",
      "Processed 82800/501501 combinations\n",
      "Processed 82900/501501 combinations\n",
      "Processed 83000/501501 combinations\n",
      "Processed 83100/501501 combinations\n",
      "Processed 83200/501501 combinations\n",
      "Processed 83300/501501 combinations\n",
      "Processed 83400/501501 combinations\n",
      "Processed 83500/501501 combinations\n",
      "Processed 83600/501501 combinations\n",
      "Processed 83700/501501 combinations\n",
      "Processed 83800/501501 combinations\n",
      "Processed 83900/501501 combinations\n",
      "Processed 84000/501501 combinations\n",
      "Processed 84100/501501 combinations\n",
      "Processed 84200/501501 combinations\n",
      "Processed 84300/501501 combinations\n",
      "Processed 84400/501501 combinations\n",
      "Processed 84500/501501 combinations\n",
      "Processed 84600/501501 combinations\n",
      "Processed 84700/501501 combinations\n",
      "Processed 84800/501501 combinations\n",
      "Processed 84900/501501 combinations\n",
      "Processed 85000/501501 combinations\n",
      "Processed 85100/501501 combinations\n",
      "Processed 85200/501501 combinations\n",
      "Processed 85300/501501 combinations\n",
      "Processed 85400/501501 combinations\n",
      "Processed 85500/501501 combinations\n",
      "Processed 85600/501501 combinations\n",
      "Processed 85700/501501 combinations\n",
      "Processed 85800/501501 combinations\n",
      "Processed 85900/501501 combinations\n",
      "Processed 86000/501501 combinations\n",
      "Processed 86100/501501 combinations\n",
      "Processed 86200/501501 combinations\n",
      "Processed 86300/501501 combinations\n",
      "Processed 86400/501501 combinations\n",
      "Processed 86500/501501 combinations\n",
      "Processed 86600/501501 combinations\n",
      "Processed 86700/501501 combinations\n",
      "Processed 86800/501501 combinations\n",
      "Processed 86900/501501 combinations\n",
      "Processed 87000/501501 combinations\n",
      "Processed 87100/501501 combinations\n",
      "Processed 87200/501501 combinations\n",
      "Processed 87300/501501 combinations\n",
      "Processed 87400/501501 combinations\n",
      "Processed 87500/501501 combinations\n",
      "Processed 87600/501501 combinations\n",
      "Processed 87700/501501 combinations\n",
      "Processed 87800/501501 combinations\n",
      "Processed 87900/501501 combinations\n",
      "Processed 88000/501501 combinations\n",
      "Processed 88100/501501 combinations\n",
      "Processed 88200/501501 combinations\n",
      "Processed 88300/501501 combinations\n",
      "Processed 88400/501501 combinations\n",
      "Processed 88500/501501 combinations\n",
      "Processed 88600/501501 combinations\n",
      "Processed 88700/501501 combinations\n",
      "Processed 88800/501501 combinations\n",
      "Processed 88900/501501 combinations\n",
      "Processed 89000/501501 combinations\n",
      "Processed 89100/501501 combinations\n",
      "Processed 89200/501501 combinations\n",
      "Processed 89300/501501 combinations\n",
      "Processed 89400/501501 combinations\n",
      "Processed 89500/501501 combinations\n",
      "Processed 89600/501501 combinations\n",
      "Processed 89700/501501 combinations\n",
      "Processed 89800/501501 combinations\n",
      "Processed 89900/501501 combinations\n",
      "Processed 90000/501501 combinations\n",
      "Processed 90100/501501 combinations\n",
      "Processed 90200/501501 combinations\n",
      "Processed 90300/501501 combinations\n",
      "Processed 90400/501501 combinations\n",
      "Processed 90500/501501 combinations\n",
      "Processed 90600/501501 combinations\n",
      "Processed 90700/501501 combinations\n",
      "Processed 90800/501501 combinations\n",
      "Processed 90900/501501 combinations\n",
      "Processed 91000/501501 combinations\n",
      "Processed 91100/501501 combinations\n",
      "Processed 91200/501501 combinations\n",
      "Processed 91300/501501 combinations\n",
      "Processed 91400/501501 combinations\n",
      "Processed 91500/501501 combinations\n",
      "Processed 91600/501501 combinations\n",
      "Processed 91700/501501 combinations\n",
      "Processed 91800/501501 combinations\n",
      "Processed 91900/501501 combinations\n",
      "Processed 92000/501501 combinations\n",
      "Processed 92100/501501 combinations\n",
      "Processed 92200/501501 combinations\n",
      "Processed 92300/501501 combinations\n",
      "Processed 92400/501501 combinations\n",
      "Processed 92500/501501 combinations\n",
      "Processed 92600/501501 combinations\n",
      "Processed 92700/501501 combinations\n",
      "Processed 92800/501501 combinations\n",
      "Processed 92900/501501 combinations\n",
      "Processed 93000/501501 combinations\n",
      "Processed 93100/501501 combinations\n",
      "Processed 93200/501501 combinations\n",
      "Processed 93300/501501 combinations\n",
      "Processed 93400/501501 combinations\n",
      "Processed 93500/501501 combinations\n",
      "Processed 93600/501501 combinations\n",
      "Processed 93700/501501 combinations\n",
      "Processed 93800/501501 combinations\n",
      "Processed 93900/501501 combinations\n",
      "Processed 94000/501501 combinations\n",
      "Processed 94100/501501 combinations\n",
      "Processed 94200/501501 combinations\n",
      "Processed 94300/501501 combinations\n",
      "Processed 94400/501501 combinations\n",
      "Processed 94500/501501 combinations\n",
      "Processed 94600/501501 combinations\n",
      "Processed 94700/501501 combinations\n",
      "Processed 94800/501501 combinations\n",
      "Processed 94900/501501 combinations\n",
      "Processed 95000/501501 combinations\n",
      "Processed 95100/501501 combinations\n",
      "Processed 95200/501501 combinations\n",
      "Processed 95300/501501 combinations\n",
      "Processed 95400/501501 combinations\n",
      "Processed 95500/501501 combinations\n",
      "Processed 95600/501501 combinations\n",
      "Processed 95700/501501 combinations\n",
      "Processed 95800/501501 combinations\n",
      "Processed 95900/501501 combinations\n",
      "Processed 96000/501501 combinations\n",
      "Processed 96100/501501 combinations\n",
      "Processed 96200/501501 combinations\n",
      "Processed 96300/501501 combinations\n",
      "Processed 96400/501501 combinations\n",
      "Processed 96500/501501 combinations\n",
      "Processed 96600/501501 combinations\n",
      "Processed 96700/501501 combinations\n",
      "Processed 96800/501501 combinations\n",
      "Processed 96900/501501 combinations\n",
      "Processed 97000/501501 combinations\n",
      "Processed 97100/501501 combinations\n",
      "Processed 97200/501501 combinations\n",
      "Processed 97300/501501 combinations\n",
      "Processed 97400/501501 combinations\n",
      "Processed 97500/501501 combinations\n",
      "Processed 97600/501501 combinations\n",
      "Processed 97700/501501 combinations\n",
      "Processed 97800/501501 combinations\n",
      "Processed 97900/501501 combinations\n",
      "Processed 98000/501501 combinations\n",
      "Processed 98100/501501 combinations\n",
      "Processed 98200/501501 combinations\n",
      "Processed 98300/501501 combinations\n",
      "Processed 98400/501501 combinations\n",
      "Processed 98500/501501 combinations\n",
      "Processed 98600/501501 combinations\n",
      "Processed 98700/501501 combinations\n",
      "Processed 98800/501501 combinations\n",
      "Processed 98900/501501 combinations\n",
      "Processed 99000/501501 combinations\n",
      "Processed 99100/501501 combinations\n",
      "Processed 99200/501501 combinations\n",
      "Processed 99300/501501 combinations\n",
      "Processed 99400/501501 combinations\n",
      "Processed 99500/501501 combinations\n",
      "Processed 99600/501501 combinations\n",
      "Processed 99700/501501 combinations\n",
      "Processed 99800/501501 combinations\n",
      "Processed 99900/501501 combinations\n",
      "Processed 100000/501501 combinations\n",
      "Processed 100100/501501 combinations\n",
      "Processed 100200/501501 combinations\n",
      "Processed 100300/501501 combinations\n",
      "Processed 100400/501501 combinations\n",
      "Processed 100500/501501 combinations\n",
      "Processed 100600/501501 combinations\n",
      "Processed 100700/501501 combinations\n",
      "Processed 100800/501501 combinations\n",
      "Processed 100900/501501 combinations\n",
      "Processed 101000/501501 combinations\n",
      "Processed 101100/501501 combinations\n",
      "Processed 101200/501501 combinations\n",
      "Processed 101300/501501 combinations\n",
      "Processed 101400/501501 combinations\n",
      "Processed 101500/501501 combinations\n",
      "Processed 101600/501501 combinations\n",
      "Processed 101700/501501 combinations\n",
      "Processed 101800/501501 combinations\n",
      "Processed 101900/501501 combinations\n",
      "Processed 102000/501501 combinations\n",
      "Processed 102100/501501 combinations\n",
      "Processed 102200/501501 combinations\n",
      "Processed 102300/501501 combinations\n",
      "Processed 102400/501501 combinations\n",
      "Processed 102500/501501 combinations\n",
      "Processed 102600/501501 combinations\n",
      "Processed 102700/501501 combinations\n",
      "Processed 102800/501501 combinations\n",
      "Processed 102900/501501 combinations\n",
      "Processed 103000/501501 combinations\n",
      "Processed 103100/501501 combinations\n",
      "Processed 103200/501501 combinations\n",
      "Processed 103300/501501 combinations\n",
      "Processed 103400/501501 combinations\n",
      "Processed 103500/501501 combinations\n",
      "Processed 103600/501501 combinations\n",
      "Processed 103700/501501 combinations\n",
      "Processed 103800/501501 combinations\n",
      "Processed 103900/501501 combinations\n",
      "Processed 104000/501501 combinations\n",
      "Processed 104100/501501 combinations\n",
      "Processed 104200/501501 combinations\n",
      "Processed 104300/501501 combinations\n",
      "Processed 104400/501501 combinations\n",
      "Processed 104500/501501 combinations\n",
      "Processed 104600/501501 combinations\n",
      "Processed 104700/501501 combinations\n",
      "Processed 104800/501501 combinations\n",
      "Processed 104900/501501 combinations\n",
      "Processed 105000/501501 combinations\n",
      "Processed 105100/501501 combinations\n",
      "Processed 105200/501501 combinations\n",
      "Processed 105300/501501 combinations\n",
      "Processed 105400/501501 combinations\n",
      "Processed 105500/501501 combinations\n",
      "Processed 105600/501501 combinations\n",
      "Processed 105700/501501 combinations\n",
      "Processed 105800/501501 combinations\n",
      "Processed 105900/501501 combinations\n",
      "Processed 106000/501501 combinations\n",
      "Processed 106100/501501 combinations\n",
      "Processed 106200/501501 combinations\n",
      "Processed 106300/501501 combinations\n",
      "Processed 106400/501501 combinations\n",
      "Processed 106500/501501 combinations\n",
      "Processed 106600/501501 combinations\n",
      "Processed 106700/501501 combinations\n",
      "Processed 106800/501501 combinations\n",
      "Processed 106900/501501 combinations\n",
      "Processed 107000/501501 combinations\n",
      "Processed 107100/501501 combinations\n",
      "Processed 107200/501501 combinations\n",
      "Processed 107300/501501 combinations\n",
      "Processed 107400/501501 combinations\n",
      "Processed 107500/501501 combinations\n",
      "Processed 107600/501501 combinations\n",
      "Processed 107700/501501 combinations\n",
      "Processed 107800/501501 combinations\n",
      "Processed 107900/501501 combinations\n",
      "Processed 108000/501501 combinations\n",
      "Processed 108100/501501 combinations\n",
      "Processed 108200/501501 combinations\n",
      "Processed 108300/501501 combinations\n",
      "Processed 108400/501501 combinations\n",
      "Processed 108500/501501 combinations\n",
      "Processed 108600/501501 combinations\n",
      "Processed 108700/501501 combinations\n",
      "Processed 108800/501501 combinations\n",
      "Processed 108900/501501 combinations\n",
      "Processed 109000/501501 combinations\n",
      "Processed 109100/501501 combinations\n",
      "Processed 109200/501501 combinations\n",
      "Processed 109300/501501 combinations\n",
      "Processed 109400/501501 combinations\n",
      "Processed 109500/501501 combinations\n",
      "Processed 109600/501501 combinations\n",
      "Processed 109700/501501 combinations\n",
      "Processed 109800/501501 combinations\n",
      "Processed 109900/501501 combinations\n",
      "Processed 110000/501501 combinations\n",
      "Processed 110100/501501 combinations\n",
      "Processed 110200/501501 combinations\n",
      "Processed 110300/501501 combinations\n",
      "Processed 110400/501501 combinations\n",
      "Processed 110500/501501 combinations\n",
      "Processed 110600/501501 combinations\n",
      "Processed 110700/501501 combinations\n",
      "Processed 110800/501501 combinations\n",
      "Processed 110900/501501 combinations\n",
      "Processed 111000/501501 combinations\n",
      "Processed 111100/501501 combinations\n",
      "Processed 111200/501501 combinations\n",
      "Processed 111300/501501 combinations\n",
      "Processed 111400/501501 combinations\n",
      "Processed 111500/501501 combinations\n",
      "Processed 111600/501501 combinations\n",
      "Processed 111700/501501 combinations\n",
      "Processed 111800/501501 combinations\n",
      "Processed 111900/501501 combinations\n",
      "Processed 112000/501501 combinations\n",
      "Processed 112100/501501 combinations\n",
      "Processed 112200/501501 combinations\n",
      "Processed 112300/501501 combinations\n",
      "Processed 112400/501501 combinations\n",
      "Processed 112500/501501 combinations\n",
      "Processed 112600/501501 combinations\n",
      "Processed 112700/501501 combinations\n",
      "Processed 112800/501501 combinations\n",
      "Processed 112900/501501 combinations\n",
      "Processed 113000/501501 combinations\n",
      "Processed 113100/501501 combinations\n",
      "Processed 113200/501501 combinations\n",
      "Processed 113300/501501 combinations\n",
      "Processed 113400/501501 combinations\n",
      "Processed 113500/501501 combinations\n",
      "Processed 113600/501501 combinations\n",
      "Processed 113700/501501 combinations\n",
      "Processed 113800/501501 combinations\n",
      "Processed 113900/501501 combinations\n",
      "Processed 114000/501501 combinations\n",
      "Processed 114100/501501 combinations\n",
      "Processed 114200/501501 combinations\n",
      "Processed 114300/501501 combinations\n",
      "Processed 114400/501501 combinations\n",
      "Processed 114500/501501 combinations\n",
      "Processed 114600/501501 combinations\n",
      "Processed 114700/501501 combinations\n",
      "Processed 114800/501501 combinations\n",
      "Processed 114900/501501 combinations\n",
      "Processed 115000/501501 combinations\n",
      "Processed 115100/501501 combinations\n",
      "Processed 115200/501501 combinations\n",
      "Processed 115300/501501 combinations\n",
      "Processed 115400/501501 combinations\n",
      "Processed 115500/501501 combinations\n",
      "Processed 115600/501501 combinations\n",
      "Processed 115700/501501 combinations\n",
      "Processed 115800/501501 combinations\n",
      "Processed 115900/501501 combinations\n",
      "Processed 116000/501501 combinations\n",
      "Processed 116100/501501 combinations\n",
      "Processed 116200/501501 combinations\n",
      "Processed 116300/501501 combinations\n",
      "Processed 116400/501501 combinations\n",
      "Processed 116500/501501 combinations\n",
      "Processed 116600/501501 combinations\n",
      "Processed 116700/501501 combinations\n",
      "Processed 116800/501501 combinations\n",
      "Processed 116900/501501 combinations\n",
      "Processed 117000/501501 combinations\n",
      "Processed 117100/501501 combinations\n",
      "Processed 117200/501501 combinations\n",
      "Processed 117300/501501 combinations\n",
      "Processed 117400/501501 combinations\n",
      "Processed 117500/501501 combinations\n",
      "Processed 117600/501501 combinations\n",
      "Processed 117700/501501 combinations\n",
      "Processed 117800/501501 combinations\n",
      "Processed 117900/501501 combinations\n",
      "Processed 118000/501501 combinations\n",
      "Processed 118100/501501 combinations\n",
      "Processed 118200/501501 combinations\n",
      "Processed 118300/501501 combinations\n",
      "Processed 118400/501501 combinations\n",
      "Processed 118500/501501 combinations\n",
      "Processed 118600/501501 combinations\n",
      "Processed 118700/501501 combinations\n",
      "Processed 118800/501501 combinations\n",
      "Processed 118900/501501 combinations\n",
      "Processed 119000/501501 combinations\n",
      "Processed 119100/501501 combinations\n",
      "Processed 119200/501501 combinations\n",
      "Processed 119300/501501 combinations\n",
      "Processed 119400/501501 combinations\n",
      "Processed 119500/501501 combinations\n",
      "Processed 119600/501501 combinations\n",
      "Processed 119700/501501 combinations\n",
      "Processed 119800/501501 combinations\n",
      "Processed 119900/501501 combinations\n",
      "Processed 120000/501501 combinations\n",
      "Processed 120100/501501 combinations\n",
      "Processed 120200/501501 combinations\n",
      "Processed 120300/501501 combinations\n",
      "Processed 120400/501501 combinations\n",
      "Processed 120500/501501 combinations\n",
      "Processed 120600/501501 combinations\n",
      "Processed 120700/501501 combinations\n",
      "Processed 120800/501501 combinations\n",
      "Processed 120900/501501 combinations\n",
      "Processed 121000/501501 combinations\n",
      "Processed 121100/501501 combinations\n",
      "Processed 121200/501501 combinations\n",
      "Processed 121300/501501 combinations\n",
      "Processed 121400/501501 combinations\n",
      "Processed 121500/501501 combinations\n",
      "Processed 121600/501501 combinations\n",
      "Processed 121700/501501 combinations\n",
      "Processed 121800/501501 combinations\n",
      "Processed 121900/501501 combinations\n",
      "Processed 122000/501501 combinations\n",
      "Processed 122100/501501 combinations\n",
      "Processed 122200/501501 combinations\n",
      "Processed 122300/501501 combinations\n",
      "Processed 122400/501501 combinations\n",
      "Processed 122500/501501 combinations\n",
      "Processed 122600/501501 combinations\n",
      "Processed 122700/501501 combinations\n",
      "Processed 122800/501501 combinations\n",
      "Processed 122900/501501 combinations\n",
      "Processed 123000/501501 combinations\n",
      "Processed 123100/501501 combinations\n",
      "Processed 123200/501501 combinations\n",
      "Processed 123300/501501 combinations\n",
      "Processed 123400/501501 combinations\n",
      "Processed 123500/501501 combinations\n",
      "Processed 123600/501501 combinations\n",
      "Processed 123700/501501 combinations\n",
      "Processed 123800/501501 combinations\n",
      "Processed 123900/501501 combinations\n",
      "Processed 124000/501501 combinations\n",
      "Processed 124100/501501 combinations\n",
      "Processed 124200/501501 combinations\n",
      "Processed 124300/501501 combinations\n",
      "Processed 124400/501501 combinations\n",
      "Processed 124500/501501 combinations\n",
      "Processed 124600/501501 combinations\n",
      "Processed 124700/501501 combinations\n",
      "Processed 124800/501501 combinations\n",
      "Processed 124900/501501 combinations\n",
      "Processed 125000/501501 combinations\n",
      "Processed 125100/501501 combinations\n",
      "Processed 125200/501501 combinations\n",
      "Processed 125300/501501 combinations\n",
      "Processed 125400/501501 combinations\n",
      "Processed 125500/501501 combinations\n",
      "Processed 125600/501501 combinations\n",
      "New best: Weights=[np.float64(0.134), np.float64(0.419), np.float64(0.447)], Accuracy=96.5000%\n",
      "Processed 125700/501501 combinations\n",
      "Processed 125800/501501 combinations\n",
      "Processed 125900/501501 combinations\n",
      "Processed 126000/501501 combinations\n",
      "Processed 126100/501501 combinations\n",
      "Processed 126200/501501 combinations\n",
      "Processed 126300/501501 combinations\n",
      "Processed 126400/501501 combinations\n",
      "Processed 126500/501501 combinations\n",
      "Processed 126600/501501 combinations\n",
      "Processed 126700/501501 combinations\n",
      "Processed 126800/501501 combinations\n",
      "Processed 126900/501501 combinations\n",
      "Processed 127000/501501 combinations\n",
      "Processed 127100/501501 combinations\n",
      "Processed 127200/501501 combinations\n",
      "Processed 127300/501501 combinations\n",
      "Processed 127400/501501 combinations\n",
      "Processed 127500/501501 combinations\n",
      "Processed 127600/501501 combinations\n",
      "Processed 127700/501501 combinations\n",
      "Processed 127800/501501 combinations\n",
      "Processed 127900/501501 combinations\n",
      "Processed 128000/501501 combinations\n",
      "Processed 128100/501501 combinations\n",
      "Processed 128200/501501 combinations\n",
      "Processed 128300/501501 combinations\n",
      "Processed 128400/501501 combinations\n",
      "Processed 128500/501501 combinations\n",
      "Processed 128600/501501 combinations\n",
      "Processed 128700/501501 combinations\n",
      "Processed 128800/501501 combinations\n",
      "Processed 128900/501501 combinations\n",
      "Processed 129000/501501 combinations\n",
      "Processed 129100/501501 combinations\n",
      "Processed 129200/501501 combinations\n",
      "Processed 129300/501501 combinations\n",
      "Processed 129400/501501 combinations\n",
      "Processed 129500/501501 combinations\n",
      "Processed 129600/501501 combinations\n",
      "Processed 129700/501501 combinations\n",
      "Processed 129800/501501 combinations\n",
      "Processed 129900/501501 combinations\n",
      "Processed 130000/501501 combinations\n",
      "Processed 130100/501501 combinations\n",
      "Processed 130200/501501 combinations\n",
      "Processed 130300/501501 combinations\n",
      "Processed 130400/501501 combinations\n",
      "Processed 130500/501501 combinations\n",
      "Processed 130600/501501 combinations\n",
      "Processed 130700/501501 combinations\n",
      "Processed 130800/501501 combinations\n",
      "Processed 130900/501501 combinations\n",
      "Processed 131000/501501 combinations\n",
      "Processed 131100/501501 combinations\n",
      "Processed 131200/501501 combinations\n",
      "Processed 131300/501501 combinations\n",
      "Processed 131400/501501 combinations\n",
      "Processed 131500/501501 combinations\n",
      "Processed 131600/501501 combinations\n",
      "Processed 131700/501501 combinations\n",
      "Processed 131800/501501 combinations\n",
      "Processed 131900/501501 combinations\n",
      "Processed 132000/501501 combinations\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     27\u001b[39m auxiliarymodel1.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33mbest_modelv2.pth\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     28\u001b[39m auxiliarymodel2.load_state_dict(torch.load(\u001b[33m'\u001b[39m\u001b[33mbest_modelv3.pth\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m best_weights, best_accuracy = \u001b[43moptimize_ensemble_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliarymodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauxiliarymodel2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_mapping\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m submission_df = predict_and_create_submission_ensemble(\n\u001b[32m     37\u001b[39m     models=[model, auxiliarymodel1, auxiliarymodel2],\n\u001b[32m     38\u001b[39m     test_loader=testloader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     weights=best_weights  \n\u001b[32m     42\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Process completed successfully! ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36moptimize_ensemble_weights\u001b[39m\u001b[34m(models, test_loader, class_mapping, submission_filename_prefix)\u001b[39m\n\u001b[32m     80\u001b[39m temp_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmission_filename_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-temp.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m submission_df.to_csv(temp_filename, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m accuracy = \u001b[43mcalculate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m accuracy > best_accuracy:\n\u001b[32m     86\u001b[39m     best_accuracy = accuracy\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mcalculate_accuracy\u001b[39m\u001b[34m(csv_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_accuracy\u001b[39m(csv_path):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     expected_labels = []\n\u001b[32m      4\u001b[39m     expected_labels.extend([\u001b[32m1\u001b[39m] * \u001b[32m50\u001b[39m)  \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\anaconda3\\envs\\oai-hutech\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\anaconda3\\envs\\oai-hutech\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\anaconda3\\envs\\oai-hutech\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\anaconda3\\envs\\oai-hutech\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\anaconda3\\envs\\oai-hutech\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:309\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, errors)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "mainingfull_programming()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oai-hutech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
